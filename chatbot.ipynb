{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-bQotCCow75",
        "outputId": "0ee74fb2-b342-4e02-c9d1-f1aa0904d3b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in c:\\users\\rohit\\anaconda3\\lib\\site-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (1.10.8)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
            "Requirement already satisfied: wget in c:\\users\\rohit\\anaconda3\\lib\\site-packages (3.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "meOW4ABlCLUL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import json\n",
        "import tqdm\n",
        "from spacy.lang.en import English\n",
        "import os\n",
        "import random\n",
        "import wget\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "KNz20a0i0lO-"
      },
      "outputs": [],
      "source": [
        "train_path = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json'\n",
        "test_path = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "f0mU32C7o9qK"
      },
      "outputs": [],
      "source": [
        "tokenizer = English()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJlN-Ujo8oNK",
        "outputId": "96c0ea8e-13a3-47e4-d28f-72bb7efd4a9c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'train-v2.0 (2).json'"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# !wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
        "\n",
        "wget.download(train_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "vRbvlGd895-M"
      },
      "outputs": [],
      "source": [
        "def open_file(file):\n",
        "  with open(file, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "  for line in lines[:10]:\n",
        "    print(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "mhKarKjAC6po"
      },
      "outputs": [],
      "source": [
        "# open_file('train-v2.0.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "vIT-zv9wDFnm"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json('train-v2.0.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "z-Vkngjtnyc9",
        "outputId": "f1218dd0-5895-4e1e-95ea-be885595b904"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>version</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Beyoncé', 'paragraphs': [{'qas': [{...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Frédéric_Chopin', 'paragraphs': [{'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Sino-Tibetan_relations_during_the_M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'IPod', 'paragraphs': [{'qas': [{'qu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'The_Legend_of_Zelda:_Twilight_Princ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  version                                               data\n",
              "0    v2.0  {'title': 'Beyoncé', 'paragraphs': [{'qas': [{...\n",
              "1    v2.0  {'title': 'Frédéric_Chopin', 'paragraphs': [{'...\n",
              "2    v2.0  {'title': 'Sino-Tibetan_relations_during_the_M...\n",
              "3    v2.0  {'title': 'IPod', 'paragraphs': [{'qas': [{'qu...\n",
              "4    v2.0  {'title': 'The_Legend_of_Zelda:_Twilight_Princ..."
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGZaAeD_Duoc",
        "outputId": "b7c0d2d7-93bf-437a-ce13-a14c3039d58e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "version    object\n",
              "data       object\n",
              "dtype: object"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pXiZKopoqIGI",
        "outputId": "d34e3d47-f738-475c-9fac-708fc5ef2415"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'r'"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['data'][0]['paragraphs'][0]['qas'][0]['question'][-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "PMJfr1v4pzDp",
        "outputId": "c1d244dc-a11f-4268-a807-075d340c3ff3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'popular'"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(df['data'][0]['paragraphs'][0]['qas'][0]['question'])[-2].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y0nff6lnLVq",
        "outputId": "a87ed23e-4c64-41db-ad69-024ee00131ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "442"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "C2g9o4L0rAsC"
      },
      "outputs": [],
      "source": [
        "# tokenize a input sentence into list of words\n",
        "def tokenize_word(sentence):\n",
        "  return [token.text for token in tokenizer(sentence)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "SXOYeqx3nOBh"
      },
      "outputs": [],
      "source": [
        "# clean the input text for the future processing\n",
        "\n",
        "def clean_text(text):\n",
        "  text = text.replace(\"]\", \" ] \")\n",
        "  text = text.replace(\"[\", \" [ \")\n",
        "  text = text.replace(\"\\n\", \" \")\n",
        "  text = text.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "wc1Up9WYrtfs"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "\n",
        "class SquadDataset(data.Dataset):\n",
        "  \"\"\"\n",
        "    Customizing squad dataset to include the following fields for each instance:\n",
        "    1. question - string describing the question\n",
        "    2. answer - string describing the corresponding answer\n",
        "    3. context - the relevent context of question and answer, not all question answer pairs have the same context\n",
        "    4. label - a higher level context, multiple question answer pairs might have same label but different context\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, question, answer, context, label):\n",
        "    self.question = question\n",
        "    self.answer = answer\n",
        "    self.context = context\n",
        "    self.label = label\n",
        "\n",
        "  def __get_item__(self, index):\n",
        "    return self.question[index], self.answer[index], self.context[index], self.label[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "xEFVma9xSPb4"
      },
      "outputs": [],
      "source": [
        "class PreprocessDataset():\n",
        "  def __init__(self, data_dir, tokenizer, max_len = 20):\n",
        "    self.data_dir = data_dir\n",
        "    self.embedding_dim = 10\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def load_data(self, file_name):\n",
        "    print(f'Loading file {file_name}...')\n",
        "    with open(file_name, 'r') as f:\n",
        "      self.data = json.load(f)\n",
        "\n",
        "  def separate_data(self, file_name):\n",
        "    self.load_data(file_name)\n",
        "    sub_dir = 'separate_data'\n",
        "\n",
        "    if not os.path.exists(os.path.join(self.data_dir, sub_dir)):\n",
        "      os.makedirs(os.path.join(self.data_dir, sub_dir))\n",
        "\n",
        "    # create a sub directory to store fields into separate files\n",
        "    with open(os.path.join(self.data_dir, sub_dir, sub_dir + '.context'), 'w', encoding=\"utf-8\") as context_file,\\\n",
        "             open(os.path.join(self.data_dir, sub_dir, sub_dir + '.question'), 'w', encoding=\"utf-8\") as question_file,\\\n",
        "             open(os.path.join(self.data_dir, sub_dir, sub_dir + '.answer'), 'w', encoding=\"utf-8\") as answer_file,\\\n",
        "             open(os.path.join(self.data_dir, sub_dir, sub_dir + '.label'), 'w', encoding=\"utf-8\") as label_file:\n",
        "      for i in tqdm.tqdm(range(len(self.data['data']))):\n",
        "        label = self.data['data'][i]['title']\n",
        "        label_token = self.tokenizer(label)\n",
        "        paragraphs = self.data['data'][i]['paragraphs']\n",
        "\n",
        "        for paragraph in paragraphs:\n",
        "          context = paragraph['context']\n",
        "          # clean context\n",
        "          context_tokens = self.tokenizer(context)\n",
        "\n",
        "          qas = paragraph['qas']\n",
        "          for qa in qas:\n",
        "            # clean qa\n",
        "            question = qa['question']\n",
        "            question_tokens = self.tokenizer(question)\n",
        "            # skip if no answers are found\n",
        "            if(qa['is_impossible'] is True):\n",
        "              continue\n",
        "            # selecting only one answer for now\n",
        "            answer = qa['answers'][0]['text']\n",
        "            answer_tokens = self.tokenizer(answer)\n",
        "\n",
        "            if(len(question_tokens) <= self.max_len and len(answer_tokens) <= self.max_len):\n",
        "              context_file.write(' '.join([token.text for token in context_tokens]) + '\\n')\n",
        "              question_file.write(' '.join([token.text for token in question_tokens]) + '\\n')\n",
        "              answer_file.write(' '.join([token.text for token in answer_tokens]) + '\\n')\n",
        "              label_file.write(''.join(label_token.text) + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwtGWwbF7BlM",
        "outputId": "96e4ae39-88db-43df-d1c0-6727f147ab95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading file train-v2.0.json...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 442/442 [00:26<00:00, 16.42it/s]\n"
          ]
        }
      ],
      "source": [
        "preprocessor = PreprocessDataset('./', tokenizer)\n",
        "preprocessor.separate_data('train-v2.0.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "# vocabulary building\n",
        "\n",
        "# giving out a unique index for these token tags\n",
        "PAD_token = 0\n",
        "BOS_token = 1\n",
        "EOS_token = 2\n",
        "UNK_token = 3\n",
        "\n",
        "PAD_tag = '<PAD>'\n",
        "BOS_tag = '<BOS>'\n",
        "EOS_tag = '<EOS>'\n",
        "UNK_tag = '<UNK>'\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, vocab_name):\n",
        "        self.vocab_name = vocab_name\n",
        "        self.trimmed = False\n",
        "        self.word2idx = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {\n",
        "            PAD_token : PAD_tag,\n",
        "            BOS_token : BOS_tag,\n",
        "            EOS_token : EOS_tag,\n",
        "            UNK_token : UNK_tag,\n",
        "        }\n",
        "        self.total_word_count = 4\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            # adding a new word to vocabulary\n",
        "            self.word2idx[word] = self.total_word_count\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.total_word_count] = word\n",
        "            self.total_word_count += 1\n",
        "        else:\n",
        "            # just increase the word count\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def trim_words(self, min_frequency):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        retain_words = []\n",
        "        for word, frequency in self.word2count.items():\n",
        "            if frequency >= min_frequency:\n",
        "                retain_words.append(word)\n",
        "\n",
        "        # re initialize data\n",
        "        self.word2idx = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {\n",
        "            PAD_token : PAD_tag,\n",
        "            BOS_token : BOS_tag,\n",
        "            EOS_token : EOS_tag,\n",
        "            UNK_token : UNK_tag,\n",
        "        }\n",
        "        self.total_word_count = 4\n",
        "\n",
        "        for word in retain_words:\n",
        "            self.addWord(word)     \n",
        "\n",
        "    def count_high_frequency_words(self, min_frequency):\n",
        "        count = 0\n",
        "        for frequency in self.word2count.values():\n",
        "            if frequency >= min_frequency:\n",
        "                count += 1\n",
        "        return count   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_vocab(vocab_name, file_paths, min_frequency=-1):\n",
        "    vocab = Vocabulary(vocab_name)\n",
        "    for file_path in file_paths:\n",
        "        sentences = open(file_path, encoding='utf-8').read().strip().split('\\n')\n",
        "        for sentence in sentences:\n",
        "            vocab.addSentence(sentence)\n",
        "        if min_frequency != -1:\n",
        "            vocab.trim_words(min_frequency)\n",
        "    return vocab\n",
        "\n",
        "def make_pairs(questions_path, answers_path):\n",
        "    pairs = []\n",
        "    with open(questions_path, encoding='utf-8') as f:\n",
        "        questions = f.read().strip().split('\\n')\n",
        "    with open(answers_path, encoding='utf-8') as f:\n",
        "        answers = f.read().strip().split('\\n')\n",
        "    for i in range(min(len(questions), len(answers))):\n",
        "        pairs.append((questions[i], answers[i]))\n",
        "    return pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "squad_vocab = create_vocab('squad_vocab', ['./separate_data/separate_data.question', './separate_data/separate_data.answer'])\n",
        "squad_pairs = make_pairs('./separate_data/separate_data.question', './separate_data/separate_data.answer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "def index_from_sentence(voc:Vocabulary, sentence:str):\n",
        "    return [voc.word2idx[word] for word in sentence.split(' ')]\n",
        "\n",
        "def pad_sentence(sentence:str, max_length = 20):\n",
        "    words = sentence.split(' ')\n",
        "    num_padding = max_length - len(words)\n",
        "    if num_padding > 0:\n",
        "        words += [PAD_tag]*num_padding\n",
        "    return ' '.join([BOS_tag] + words + [EOS_tag])\n",
        "\n",
        "def pad_sentence_batch(sentences, pad_token = PAD_token):\n",
        "    return list(itertools.zip_longest(*sentences, fillvalue = pad_token))\n",
        "\n",
        "def pad_mask(sentences, mask_token = PAD_token):\n",
        "    sentences_mask = []\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        sentence_mask = []\n",
        "        for token in sentence:\n",
        "            if token == mask_token:\n",
        "                sentence_mask.append(1)\n",
        "            else:\n",
        "                sentence_mask.append(0)\n",
        "        sentences_mask.append(sentence_mask)\n",
        "    return sentences_mask\n",
        "\n",
        "\n",
        "def batch_input_sentences(sentences, voc: Vocabulary):\n",
        "    '''\n",
        "        1. convert the string sentence into a list of numbers - each number is the index of a word \n",
        "           according to the vocabulary voc.\n",
        "        2. pad these sentences - more description in pad_sentence_batch function\n",
        "        3. return the padded - indexed - batched sentence and the length of individual sentences\n",
        "    '''\n",
        "    indexed_sentences = [index_from_sentence(voc, sentence) for sentence in sentences]\n",
        "    sentence_lengths = [ len(sentence) for sentence in indexed_sentences]\n",
        "    padded_batch = pad_sentence_batch(indexed_sentences)   \n",
        "    return torch.LongTensor(padded_batch), torch.tensor(sentence_lengths)\n",
        "\n",
        "def batch_output_sentences(sentences, voc: Vocabulary):\n",
        "    indexed_sentences = [index_from_sentence(voc, sentence) for sentence in sentences]\n",
        "    max_sentence_length = max([len(sentence) for sentence in indexed_sentences])\n",
        "    padded_batch = pad_sentence_batch(indexed_sentences)\n",
        "    batch_mask = pad_mask(padded_batch)\n",
        "    batch_mask = torch.BoolTensor(batch_mask)\n",
        "    return torch.LongTensor(padded_batch), batch_mask, max_sentence_length\n",
        "\n",
        "def batch_2_train_data(voc: Vocabulary, pairs):\n",
        "    pairs.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pairs:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    return batch_input_sentences(input_batch, voc), batch_output_sentences(output_batch, voc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mask: tensor([[False, False, False, False, False],\n",
            "        [False, False, False, False, False],\n",
            "        [False, False, False, False, False],\n",
            "        [False, False, False, False, False],\n",
            "        [False, False, False, False,  True],\n",
            "        [ True, False, False,  True,  True],\n",
            "        [ True, False,  True,  True,  True],\n",
            "        [ True, False,  True,  True,  True],\n",
            "        [ True, False,  True,  True,  True],\n",
            "        [ True, False,  True,  True,  True]])\n"
          ]
        }
      ],
      "source": [
        "# Example for validation\n",
        "small_batch_size = 5\n",
        "batches = batch_2_train_data(squad_vocab, [random.choice(squad_pairs) for _ in range(small_batch_size)])\n",
        "(input_variable, lengths), (target_variable, mask, max_target_len) = batches\n",
        "\n",
        "# print(\"input_variable:\", input_variable)\n",
        "# print(\"lengths:\", lengths)\n",
        "# print(\"target_variable:\", target_variable)\n",
        "print(\"mask:\", mask)\n",
        "# print(\"max_target_len:\", max_target_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[62, 448, 5, 40, 71, 154, 414, 450, 14, 453, 10]"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index_from_sentence(squad_vocab, 'How much did the second world tour make in dollars ?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<BOS> How much did the second world tour make in dollars ? <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <EOS>'"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pad_sentence('How much did the second world tour make in dollars ?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "61071"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "squad_vocab.total_word_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19048"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "squad_vocab.count_high_frequency_words(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "# since seq-seq model is encoder decoder type \n",
        "# we'll be having two model classes - one for encoder and other for decoder\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, num_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers,\n",
        "                          dropout=(0 if num_layers == 1 else dropout), bidirectional=True)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for name, param in self.gru.named_parameters():\n",
        "            if 'weight_ih' in name:\n",
        "                torch.nn.init.xavier_uniform_(param.data)\n",
        "            elif 'weight_hh' in name:\n",
        "                torch.nn.init.orthogonal_(param.data)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        embedded = self.embedding(input_seq)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
        "        return outputs, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.zeros(hidden_size))\n",
        "            nn.init.xavier_uniform_(self.v.data)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        if self.method in ['general', 'concat']:\n",
        "            nn.init.xavier_uniform_(self.attn.weight)\n",
        "            nn.init.constant_(self.attn.bias, 0)\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        attn_energies = attn_energies.t()\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, num_layers=1, dropout=0.1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        # Keep for reference\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Define layers\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, dropout=(0 if num_layers == 1 else dropout))\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step (word) at a time\n",
        "        # Get embedding of current input word\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        # Forward through unidirectional GRU\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "        # Calculate attention weights from the current GRU output\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        # Predict next word using Luong eq. 6\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        # Return output and final hidden state\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def maskNLLLoss(inp, target, mask, device='cpu'):\n",
        "#     nTotal = mask.sum()\n",
        "#     crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "#     masked_cross_entropy = crossEntropy.masked_select(mask)\n",
        "#     loss = masked_cross_entropy.mean()\n",
        "#     loss = loss.to(device)  # Ensure the loss tensor is on the correct device\n",
        "#     print(f'Loss: {loss}, nTotal: {nTotal.item()}')\n",
        "#     return loss, nTotal.item()\n",
        "\n",
        "\n",
        "def maskNLLLoss(inp, target, mask):\n",
        "    print(f'input: {inp}, target: {target}, mask: {mask}')\n",
        "    nTotal = mask.sum()\n",
        "    print(f'ntotal: {nTotal}')\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    print(f'crossEntropy: {crossEntropy}')\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    print(f'loss: {loss}, ntotal item: {nTotal.item()}')\n",
        "    return loss, nTotal.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 4000\n",
        "print_every = 1\n",
        "save_every = 500\n",
        "checkpoint_path = 'model.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
        "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=20):\n",
        "\n",
        "    # Zero gradients\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # Set device options\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # Lengths for RNN packing should always be on the CPU\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    # Initialize variables\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    # Forward pass through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
        "    decoder_input = torch.LongTensor([[BOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    decoder_hidden = encoder_hidden[:decoder.num_layers]\n",
        "\n",
        "    # Determine if we are using teacher forcing this iteration\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # Forward batch of sequences through decoder one time step at a time\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # Teacher forcing: next input is current target\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # No teacher forcing: next input is decoder's own current output\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    # Perform backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients: gradients are modified in place\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    # Adjust model weights\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure models\n",
        "model_name = 'cb_model'\n",
        "attn_model = 'dot'\n",
        "#``attn_model = 'general'``\n",
        "#``attn_model = 'concat'``\n",
        "hidden_size = 500\n",
        "encoder_num_layers = 2\n",
        "decoder_num_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "# Set checkpoint to load from; set to None if starting from scratch\n",
        "loadFilename = None\n",
        "checkpoint_iter = 4000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building encoder and decoder ...\n",
            "Models built and ready to go!\n"
          ]
        }
      ],
      "source": [
        "# Load model if a ``loadFilename`` is provided\n",
        "if loadFilename:\n",
        "    # If loading on same machine the model was trained on\n",
        "    checkpoint = torch.load(loadFilename)\n",
        "    # If loading a model trained on GPU to CPU\n",
        "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "    squad_vocab.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "# Initialize word embeddings\n",
        "embedding = nn.Embedding(squad_vocab.total_word_count, hidden_size)\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "# Initialize encoder & decoder models\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_num_layers, dropout)\n",
        "decoder = DecoderRNN(attn_model, embedding, hidden_size, squad_vocab.total_word_count, decoder_num_layers, dropout)\n",
        "if loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "# Use appropriate device\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print('Models built and ready to go!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_num_layers, decoder_num_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
        "\n",
        "    # Load batches for each iteration\n",
        "    training_batches = [batch_2_train_data(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "                      for _ in range(n_iteration)]\n",
        "\n",
        "    # Initializations\n",
        "    print('Initializing ...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "    if loadFilename:\n",
        "        start_iteration = checkpoint['iteration'] + 1\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        # Extract fields from batch\n",
        "        (input_variable, lengths), (target_variable, mask, max_target_len) = training_batch\n",
        "\n",
        "        # Run a training iteration with batch\n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
        "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "        print_loss += loss\n",
        "\n",
        "        # Print progress\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "            print_loss = 0\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (iteration % save_every == 0):\n",
        "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_num_layers, decoder_num_layers, hidden_size))\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': voc.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building optimizers ...\n",
            "Starting Training!\n",
            "Initializing ...\n",
            "Training...\n",
            "input: tensor([[1.9277e-05, 1.5425e-05, 1.8336e-05,  ..., 1.6551e-05, 1.6097e-05,\n",
            "         1.6571e-05],\n",
            "        [1.7125e-05, 1.5960e-05, 1.6504e-05,  ..., 1.4276e-05, 1.6593e-05,\n",
            "         1.5605e-05],\n",
            "        [1.7542e-05, 1.5141e-05, 1.7152e-05,  ..., 1.6833e-05, 1.6844e-05,\n",
            "         1.6282e-05],\n",
            "        ...,\n",
            "        [1.6273e-05, 1.6933e-05, 1.7215e-05,  ..., 1.6803e-05, 1.6515e-05,\n",
            "         1.5631e-05],\n",
            "        [1.7613e-05, 1.4947e-05, 1.7267e-05,  ..., 1.7244e-05, 1.7050e-05,\n",
            "         1.7150e-05],\n",
            "        [1.6721e-05, 1.7373e-05, 1.8328e-05,  ..., 1.7833e-05, 1.5544e-05,\n",
            "         1.6691e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([26794,   345, 35257,  4968, 16775,  1520, 50370,  2362,    26,  1476,\n",
            "           40,   140, 34299,  8418,  4425,  8314, 36284,  1840,  1449,  8030,\n",
            "         1108,   772, 12639, 13864,   275, 12233,  7803, 12496,  4152, 50559,\n",
            "        25136,  2343,  9438, 51295,   832, 29870, 17354,  6868, 17249, 10341,\n",
            "         1696,  3198,   230,   341,   496,  8544, 40518,  9969,  4286, 45978,\n",
            "         1271, 16895, 36030,   598,  3052,  2001,    40,  1590,  1249, 24016,\n",
            "         6695, 60805,   679, 44498]), mask: tensor([False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False])\n",
            "ntotal: 0\n",
            "crossEntropy: tensor([11.0194, 11.0216, 10.8777, 11.0750, 11.0121, 10.9478, 10.9255, 10.9895,\n",
            "        11.0285, 11.1495, 10.9485, 11.1196, 10.9712, 11.1026, 10.9960, 11.0358,\n",
            "        11.0700, 10.9178, 10.9910, 11.1263, 11.1611, 11.0074, 11.0386, 11.0288,\n",
            "        11.1881, 11.0991, 11.0724, 11.0221, 11.0092, 11.0426, 11.0137, 10.9637,\n",
            "        11.0051, 10.9783, 10.9874, 10.9769, 10.9174, 10.9272, 10.9939, 11.0822,\n",
            "        10.9626, 11.0560, 10.9731, 10.9667, 10.9718, 10.9886, 10.9739, 11.0180,\n",
            "        10.9870, 11.0844, 10.9220, 10.9922, 11.0563, 11.0576, 10.8814, 11.0527,\n",
            "        11.0656, 10.9804, 10.9759, 10.9697, 10.9616, 10.9696, 10.9951, 11.0894],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: nan, ntotal item: 0\n",
            "input: tensor([[1.8969e-05, 1.5475e-05, 1.8154e-05,  ..., 1.6581e-05, 1.6081e-05,\n",
            "         1.5755e-05],\n",
            "        [1.6631e-05, 1.6222e-05, 1.6499e-05,  ..., 1.5312e-05, 1.6741e-05,\n",
            "         1.5693e-05],\n",
            "        [1.7400e-05, 1.5889e-05, 1.7026e-05,  ..., 1.6749e-05, 1.6201e-05,\n",
            "         1.6442e-05],\n",
            "        ...,\n",
            "        [1.6402e-05, 1.6306e-05, 1.7599e-05,  ..., 1.6758e-05, 1.6626e-05,\n",
            "         1.5101e-05],\n",
            "        [1.7041e-05, 1.5193e-05, 1.6744e-05,  ..., 1.7314e-05, 1.7216e-05,\n",
            "         1.6961e-05],\n",
            "        [1.6925e-05, 1.6670e-05, 1.7680e-05,  ..., 1.7685e-05, 1.5928e-05,\n",
            "         1.6542e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([  112,     0,     0,     0,  1592,  1523,     0,    69, 14752,     0,\n",
            "        51908, 11083,  1202,    24,    69, 12881,  1519,     0,   115,     0,\n",
            "          350, 57033,     0, 50549, 20148,  3448,  4498, 14497,  1958,     0,\n",
            "            0,     0,     0,     0,     0, 29871,     0,   652,     0,     0,\n",
            "           22,  5035,     0,     0, 25090,     0,   864,     0,   115, 36661,\n",
            "           69,     0,  2113,   112,  4521,  5215, 53713,     0,     0,  7299,\n",
            "         3936, 13285,   115,     0]), mask: tensor([False,  True,  True,  True, False, False,  True, False, False,  True,\n",
            "        False, False, False, False, False, False, False,  True, False,  True,\n",
            "        False, False,  True, False, False, False, False, False, False,  True,\n",
            "         True,  True,  True,  True,  True, False,  True, False,  True,  True,\n",
            "        False, False,  True,  True, False,  True, False,  True, False, False,\n",
            "        False,  True, False, False, False, False, False,  True,  True, False,\n",
            "        False, False, False,  True])\n",
            "ntotal: 25\n",
            "crossEntropy: tensor([11.0221, 11.0042, 10.9590, 11.0546, 10.9059, 11.0079, 10.9906, 10.9638,\n",
            "        10.9565, 11.0110, 10.9134, 10.9568, 11.0229, 10.9585, 11.0053, 11.0879,\n",
            "        11.0292, 10.9415, 10.9841, 11.0047, 11.0678, 10.9434, 10.9758, 11.1028,\n",
            "        10.8387, 11.0013, 10.9709, 10.9347, 11.0321, 10.9637, 10.9915, 11.0246,\n",
            "        10.9554, 11.0004, 10.9794, 10.9910, 11.0471, 11.1120, 11.0655, 11.0180,\n",
            "        11.0968, 11.0906, 11.0005, 11.0264, 11.0045, 10.9874, 10.9850, 10.9763,\n",
            "        10.9861, 11.0519, 10.9844, 11.0752, 11.0599, 11.1111, 11.1068, 11.0537,\n",
            "        11.0390, 11.0182, 11.0001, 10.9606, 11.0072, 10.9370, 11.0115, 10.9867],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 11.002315521240234, ntotal item: 25\n",
            "input: tensor([[1.8690e-05, 1.6072e-05, 1.7265e-05,  ..., 1.6469e-05, 1.6252e-05,\n",
            "         1.5569e-05],\n",
            "        [1.6675e-05, 1.6708e-05, 1.7330e-05,  ..., 1.5437e-05, 1.6159e-05,\n",
            "         1.6361e-05],\n",
            "        [1.7475e-05, 1.5988e-05, 1.7696e-05,  ..., 1.6665e-05, 1.5817e-05,\n",
            "         1.7058e-05],\n",
            "        ...,\n",
            "        [1.6246e-05, 1.5724e-05, 1.7755e-05,  ..., 1.6862e-05, 1.6742e-05,\n",
            "         1.5710e-05],\n",
            "        [1.6671e-05, 1.5558e-05, 1.7623e-05,  ..., 1.7066e-05, 1.6595e-05,\n",
            "         1.6656e-05],\n",
            "        [1.6378e-05, 1.6375e-05, 1.7586e-05,  ..., 1.7122e-05, 1.6247e-05,\n",
            "         1.7000e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([   40,     0,     0,     0, 58078,  8597,     0,  3851,  5091,     0,\n",
            "            0, 11084,   453,  4188,  1628, 39898,    14,     0, 28638,     0,\n",
            "           26,     0,     0, 26564,  1170,     0,  5611,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0, 10103,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0, 42295,     0,    40,     0,\n",
            "         1281,     0,  9471, 12205,     0,    97, 28086,     0,     0,     0,\n",
            "            0,     0,  4217,     0]), mask: tensor([False,  True,  True,  True, False, False,  True, False, False,  True,\n",
            "         True, False, False, False, False, False, False,  True, False,  True,\n",
            "        False,  True,  True, False, False,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True, False,  True, False,  True,\n",
            "        False,  True, False, False,  True, False, False,  True,  True,  True,\n",
            "         True,  True, False,  True])\n",
            "ntotal: 39\n",
            "crossEntropy: tensor([10.9957, 11.0016, 10.9548, 11.0441, 11.0079, 11.0035, 11.0062, 11.0934,\n",
            "        11.0871, 11.0402, 10.9646, 10.9764, 11.0562, 11.0367, 11.0361, 11.0578,\n",
            "        11.0155, 10.9542, 10.9897, 10.9958, 11.0780, 10.9819, 11.0190, 10.9634,\n",
            "        11.0104, 11.0197, 11.0870, 11.0033, 10.9678, 10.9703, 10.9909, 11.0124,\n",
            "        10.9669, 10.9961, 10.9990, 11.0772, 11.0096, 11.1796, 11.0832, 11.0049,\n",
            "        11.0116, 11.0514, 11.0164, 11.0397, 10.9628, 10.9924, 10.9365, 10.9607,\n",
            "        11.0208, 11.0108, 11.1342, 11.0805, 11.0225, 11.0868, 10.9526, 11.0155,\n",
            "        11.0765, 11.0008, 10.9923, 10.9271, 10.9804, 11.0277, 11.0482, 11.0195],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 11.002312660217285, ntotal item: 39\n",
            "input: tensor([[1.8579e-05, 1.5993e-05, 1.6821e-05,  ..., 1.6213e-05, 1.6087e-05,\n",
            "         1.5818e-05],\n",
            "        [1.6760e-05, 1.7240e-05, 1.7599e-05,  ..., 1.5651e-05, 1.5891e-05,\n",
            "         1.7090e-05],\n",
            "        [1.7237e-05, 1.6139e-05, 1.7918e-05,  ..., 1.6420e-05, 1.5597e-05,\n",
            "         1.7642e-05],\n",
            "        ...,\n",
            "        [1.6227e-05, 1.5740e-05, 1.8297e-05,  ..., 1.6432e-05, 1.6619e-05,\n",
            "         1.6107e-05],\n",
            "        [1.6933e-05, 1.5807e-05, 1.7060e-05,  ..., 1.6983e-05, 1.6771e-05,\n",
            "         1.6240e-05],\n",
            "        [1.6250e-05, 1.6287e-05, 1.7862e-05,  ..., 1.6483e-05, 1.6182e-05,\n",
            "         1.7377e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([13587,     0,     0,     0,     0,     0,     0,     0,  3557,     0,\n",
            "            0, 11085,  5699,  9859,     0,  1312, 23273,     0,     0,     0,\n",
            "         1202,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,  1271,     0,   115,     0,\n",
            "          767,     0,     0,    52,     0,    40,  4678,     0,     0,     0,\n",
            "            0,     0,     0,     0]), mask: tensor([False,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True, False, False, False,  True, False, False,  True,  True,  True,\n",
            "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True, False,  True, False,  True,\n",
            "        False,  True,  True, False,  True, False, False,  True,  True,  True,\n",
            "         True,  True,  True,  True])\n",
            "ntotal: 50\n",
            "crossEntropy: tensor([11.0845, 10.9965, 10.9685, 11.0455, 11.0125, 10.9169, 11.0105, 10.9773,\n",
            "        10.9065, 11.0733, 10.9987, 10.9422, 11.1032, 11.0261, 10.9749, 11.0567,\n",
            "        10.9311, 10.9637, 11.0132, 10.9995, 10.9848, 10.9898, 11.0342, 11.0244,\n",
            "        11.0076, 11.0167, 11.0643, 10.9822, 10.9742, 10.9688, 10.9982, 11.0159,\n",
            "        10.9733, 10.9960, 11.0445, 11.1008, 11.0269, 10.9560, 11.0966, 11.0094,\n",
            "        10.9866, 11.0613, 11.0232, 11.0414, 10.9615, 11.0071, 11.0198, 10.9560,\n",
            "        10.9741, 11.0289, 11.0492, 11.0841, 10.9906, 11.0073, 10.9722, 10.9994,\n",
            "        11.0948, 11.0282, 11.0025, 10.9467, 10.9774, 11.0288, 10.9862, 11.0274],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 11.006824493408203, ntotal item: 50\n",
            "input: tensor([[1.8568e-05, 1.5795e-05, 1.7080e-05,  ..., 1.6260e-05, 1.5864e-05,\n",
            "         1.6038e-05],\n",
            "        [1.6491e-05, 1.7846e-05, 1.8156e-05,  ..., 1.5619e-05, 1.5653e-05,\n",
            "         1.7331e-05],\n",
            "        [1.6800e-05, 1.6385e-05, 1.8509e-05,  ..., 1.6297e-05, 1.5453e-05,\n",
            "         1.8135e-05],\n",
            "        ...,\n",
            "        [1.6038e-05, 1.6450e-05, 1.9006e-05,  ..., 1.5906e-05, 1.6764e-05,\n",
            "         1.6446e-05],\n",
            "        [1.7044e-05, 1.6588e-05, 1.7546e-05,  ..., 1.6389e-05, 1.6435e-05,\n",
            "         1.7014e-05],\n",
            "        [1.6119e-05, 1.6350e-05, 1.8079e-05,  ..., 1.6308e-05, 1.5831e-05,\n",
            "         1.7890e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([12588,     0,     0,     0,     0,     0,     0,     0,   298,     0,\n",
            "            0,     0, 44041,     0,     0,     0,     0,     0,     0,     0,\n",
            "          151,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,  1074,     0,\n",
            "            0,     0,     0,  4869,     0,  1089,  4657,     0,     0,     0,\n",
            "            0,     0,     0,     0]), mask: tensor([False,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True,  True,  True, False,  True, False, False,  True,  True,  True,\n",
            "         True,  True,  True,  True])\n",
            "ntotal: 56\n",
            "crossEntropy: tensor([11.0535, 11.0127, 10.9941, 11.0331, 11.0031, 10.9320, 11.0398, 10.9970,\n",
            "        11.0508, 11.0892, 11.0423, 11.0025, 11.0183, 10.9869, 10.9740, 11.0379,\n",
            "        10.9945, 10.9740, 11.0186, 10.9995, 10.9625, 11.0090, 11.0418, 11.0171,\n",
            "        11.0056, 11.0210, 11.0686, 10.9901, 11.0113, 10.9809, 11.0110, 11.0348,\n",
            "        10.9721, 11.0045, 11.0688, 11.1091, 11.0314, 10.9674, 11.1133, 11.0175,\n",
            "        11.0040, 11.0363, 11.0273, 11.0621, 10.9778, 11.0158, 10.9610, 10.9775,\n",
            "        10.9475, 11.0319, 10.9835, 11.1101, 10.9995, 10.9848, 11.0005, 10.9342,\n",
            "        11.0061, 11.0250, 11.0064, 10.9699, 10.9969, 11.0405, 10.9797, 11.0355],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 11.01513385772705, ntotal item: 56\n",
            "input: tensor([[1.8139e-05, 1.5942e-05, 1.7131e-05,  ..., 1.6171e-05, 1.5490e-05,\n",
            "         1.5953e-05],\n",
            "        [1.6392e-05, 1.7802e-05, 1.8308e-05,  ..., 1.5447e-05, 1.5518e-05,\n",
            "         1.7873e-05],\n",
            "        [1.6621e-05, 1.6678e-05, 1.8704e-05,  ..., 1.6067e-05, 1.5528e-05,\n",
            "         1.8497e-05],\n",
            "        ...,\n",
            "        [1.6104e-05, 1.6631e-05, 1.8969e-05,  ..., 1.5988e-05, 1.6437e-05,\n",
            "         1.6421e-05],\n",
            "        [1.6781e-05, 1.7064e-05, 1.8324e-05,  ..., 1.6002e-05, 1.6363e-05,\n",
            "         1.7179e-05],\n",
            "        [1.6129e-05, 1.7177e-05, 1.8593e-05,  ..., 1.5963e-05, 1.5808e-05,\n",
            "         1.8065e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([   69,     0,     0,     0,     0,     0,     0,     0, 52005,     0,\n",
            "            0,     0,  1202,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0, 10367,     0,     0,     0,\n",
            "            0,     0,     0,     0]), mask: tensor([False,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True, False,  True,  True,  True,\n",
            "         True,  True,  True,  True])\n",
            "ntotal: 60\n",
            "crossEntropy: tensor([11.0272, 11.0187, 11.0049, 11.0583, 11.0055, 10.9503, 11.0605, 11.0125,\n",
            "        10.9858, 11.1024, 11.0547, 11.0004, 11.0578, 10.9787, 10.9724, 11.0404,\n",
            "        11.0141, 10.9803, 11.0202, 11.0089, 10.9856, 11.0036, 11.0645, 11.0303,\n",
            "        11.0082, 11.0300, 11.0600, 11.0120, 11.0112, 11.0074, 11.0244, 11.0368,\n",
            "        10.9851, 11.0125, 11.0685, 11.1233, 11.0412, 10.9884, 11.1028, 11.0267,\n",
            "        11.0224, 11.0300, 11.0278, 11.0390, 11.0071, 11.0329, 10.9664, 11.0161,\n",
            "        11.0208, 11.0613, 11.0026, 11.1116, 11.0223, 10.9515, 11.0455, 11.0283,\n",
            "        11.1153, 11.0394, 11.0111, 10.9822, 11.0260, 11.0364, 10.9953, 11.0349],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 11.023611068725586, ntotal item: 60\n",
            "input: tensor([[1.8637e-05, 1.5918e-05, 1.7365e-05,  ..., 1.6447e-05, 1.5548e-05,\n",
            "         1.6278e-05],\n",
            "        [1.6050e-05, 1.8030e-05, 1.8903e-05,  ..., 1.5468e-05, 1.5461e-05,\n",
            "         1.7842e-05],\n",
            "        [1.6436e-05, 1.6622e-05, 1.8770e-05,  ..., 1.5918e-05, 1.5467e-05,\n",
            "         1.8360e-05],\n",
            "        ...,\n",
            "        [1.5782e-05, 1.7234e-05, 1.9563e-05,  ..., 1.5580e-05, 1.6130e-05,\n",
            "         1.6334e-05],\n",
            "        [1.6209e-05, 1.7322e-05, 1.8450e-05,  ..., 1.5807e-05, 1.6322e-05,\n",
            "         1.7063e-05],\n",
            "        [1.5988e-05, 1.6742e-05, 1.8559e-05,  ..., 1.6338e-05, 1.6047e-05,\n",
            "         1.8361e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([ 6034,     0,     0,     0,     0,     0,     0,     0,  1397,     0,\n",
            "            0,     0, 52562,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0]), mask: tensor([False,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True])\n",
            "ntotal: 61\n",
            "crossEntropy: tensor([11.0263, 11.0398, 11.0160, 11.0475, 11.0139, 10.9788, 11.0731, 11.0057,\n",
            "        10.9959, 11.1146, 11.0689, 11.0026, 11.0431, 10.9776, 10.9858, 11.0372,\n",
            "        11.0373, 10.9866, 11.0501, 11.0029, 10.9969, 11.0062, 11.0481, 11.0441,\n",
            "        11.0214, 11.0218, 11.0700, 11.0451, 11.0329, 11.0014, 11.0293, 11.0428,\n",
            "        11.0168, 11.0131, 11.0780, 11.1237, 11.0452, 11.0112, 11.1010, 11.0339,\n",
            "        11.0171, 11.0349, 11.0384, 11.0463, 11.0295, 11.0493, 10.9751, 11.0264,\n",
            "        11.0267, 11.0795, 11.0080, 11.1334, 11.0521, 10.9836, 11.0415, 11.0177,\n",
            "        11.0521, 11.0258, 11.0430, 10.9850, 11.0299, 11.0566, 11.0300, 11.0436],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 11.033553123474121, ntotal item: 61\n",
            "input: tensor([[1.8432e-05, 1.6760e-05, 1.8076e-05,  ..., 1.6414e-05, 1.5219e-05,\n",
            "         1.6161e-05],\n",
            "        [1.5897e-05, 1.8083e-05, 1.9318e-05,  ..., 1.5467e-05, 1.5441e-05,\n",
            "         1.8130e-05],\n",
            "        [1.6359e-05, 1.6791e-05, 1.8568e-05,  ..., 1.6349e-05, 1.5435e-05,\n",
            "         1.8562e-05],\n",
            "        ...,\n",
            "        [1.5319e-05, 1.6997e-05, 1.9477e-05,  ..., 1.5242e-05, 1.6114e-05,\n",
            "         1.6559e-05],\n",
            "        [1.6421e-05, 1.6943e-05, 1.8491e-05,  ..., 1.5913e-05, 1.6087e-05,\n",
            "         1.7302e-05],\n",
            "        [1.6004e-05, 1.6673e-05, 1.8684e-05,  ..., 1.6079e-05, 1.6090e-05,\n",
            "         1.8006e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "        5702,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0]), mask: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True])\n",
            "ntotal: 63\n",
            "crossEntropy: tensor([10.9014, 11.0494, 11.0207, 11.0501, 11.0464, 10.9916, 11.0678, 11.0268,\n",
            "        10.9321, 11.1147, 11.0749, 11.0223, 11.0874, 10.9786, 10.9979, 11.0538,\n",
            "        11.0421, 11.0036, 11.0340, 11.0077, 11.0150, 11.0222, 11.0756, 11.0418,\n",
            "        11.0372, 11.0231, 11.0610, 11.0899, 11.0304, 11.0408, 11.0234, 11.0625,\n",
            "        11.0053, 11.0186, 11.0755, 11.1346, 11.0582, 11.0264, 11.0849, 11.0312,\n",
            "        11.0093, 11.0195, 11.0541, 11.0461, 11.0287, 11.0638, 10.9876, 11.0424,\n",
            "        11.0596, 11.0858, 11.0182, 11.1109, 11.0566, 11.0073, 11.0495, 11.0262,\n",
            "        11.0473, 11.0379, 11.0587, 11.0199, 11.0439, 11.0864, 11.0169, 11.0427],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 11.037981033325195, ntotal item: 63\n",
            "Iteration: 1; Percent complete: 0.0%; Average loss: nan\n",
            "input: tensor([[2.7928e-05, 1.4730e-05, 1.7190e-05,  ..., 1.7145e-05, 1.6460e-05,\n",
            "         1.5627e-05],\n",
            "        [2.5107e-05, 1.5571e-05, 1.7475e-05,  ..., 1.6777e-05, 1.5813e-05,\n",
            "         1.7013e-05],\n",
            "        [3.0046e-05, 1.5945e-05, 1.7187e-05,  ..., 1.5925e-05, 1.6572e-05,\n",
            "         1.5875e-05],\n",
            "        ...,\n",
            "        [2.5902e-05, 1.7616e-05, 1.8668e-05,  ..., 1.6844e-05, 1.5572e-05,\n",
            "         1.7123e-05],\n",
            "        [2.6110e-05, 1.7758e-05, 1.7395e-05,  ..., 1.7534e-05, 1.5217e-05,\n",
            "         1.6053e-05],\n",
            "        [2.2058e-05, 1.6033e-05, 1.8077e-05,  ..., 1.8251e-05, 1.6354e-05,\n",
            "         1.7125e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([27994,  2571,   466,  8395,   331, 14822, 46428, 20973,  2759,   578,\n",
            "         3248, 32096, 51385, 49929, 40716, 17122,  6669, 10445,  5547,  3465,\n",
            "        19503,  7659, 49456, 34049, 12702, 28026, 14971, 46600, 38830,  3596,\n",
            "        52138,   429, 42699, 16456, 56366, 26476, 50762,    40,  1519,    40,\n",
            "         5220, 18531, 47796, 45531, 46116,  1282,  2145,   346, 60337,  2383,\n",
            "         9701,  1925, 33538,   933,  7947,   517,    40, 59517, 37466,  6832,\n",
            "          127, 60686, 20269, 24180]), mask: tensor([False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False, False, False, False, False, False, False,\n",
            "        False, False, False, False])\n",
            "ntotal: 0\n",
            "crossEntropy: tensor([10.9478, 11.0228, 11.1253, 11.0595, 11.0806, 10.9915, 10.9526, 11.0373,\n",
            "        10.9874, 11.1261, 11.1217, 10.9528, 11.1484, 11.1825, 11.0456, 10.9842,\n",
            "        10.9754, 10.9604, 11.0770, 10.9121, 11.0436, 11.1317, 11.0451, 11.1659,\n",
            "        10.9791, 11.0538, 10.9760, 11.0276, 11.0729, 11.0048, 10.8905, 11.0269,\n",
            "        11.1187, 11.1207, 11.0414, 10.9304, 11.1114, 11.0367, 11.0256, 10.9913,\n",
            "        10.9591, 11.0420, 10.9516, 11.0425, 11.0211, 11.0668, 11.1100, 11.1203,\n",
            "        11.0611, 11.0218, 11.1809, 11.0120, 11.0170, 11.0014, 11.0106, 11.1042,\n",
            "        11.0395, 11.0423, 11.0507, 10.9644, 11.0155, 11.0429, 11.0529, 11.0225],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: nan, ntotal item: 0\n",
            "input: tensor([[2.8020e-05, 1.5191e-05, 1.7465e-05,  ..., 1.7398e-05, 1.7122e-05,\n",
            "         1.4881e-05],\n",
            "        [2.5929e-05, 1.5818e-05, 1.7446e-05,  ..., 1.7469e-05, 1.6208e-05,\n",
            "         1.5915e-05],\n",
            "        [2.9270e-05, 1.6229e-05, 1.7456e-05,  ..., 1.6306e-05, 1.6045e-05,\n",
            "         1.5732e-05],\n",
            "        ...,\n",
            "        [2.4466e-05, 1.6979e-05, 1.7988e-05,  ..., 1.7443e-05, 1.6040e-05,\n",
            "         1.6917e-05],\n",
            "        [2.6267e-05, 1.7187e-05, 1.7165e-05,  ..., 1.7565e-05, 1.5473e-05,\n",
            "         1.5646e-05],\n",
            "        [2.3094e-05, 1.6229e-05, 1.7400e-05,  ..., 1.8218e-05, 1.6776e-05,\n",
            "         1.7101e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([    0,     0,     0,  2362,   133, 14823,     0, 17198, 22920, 14005,\n",
            "           69,     0,   418, 49930,  1970,     0,    24,     0,  1592, 27991,\n",
            "            0, 12563,  7299,    72,   151,     0,     0,   115,     0, 36747,\n",
            "            0,     0, 42700,     0, 16939, 60108, 50763,   397,  4231,  6599,\n",
            "           72,  1970,     0, 11535, 46117,  2817,     0,     0,  3877,  2376,\n",
            "          274,    52,     0,   519,  8086,   118,  8309,     0, 58302,   122,\n",
            "        21378, 60687, 20270,     0]), mask: tensor([ True,  True,  True, False, False, False,  True, False, False, False,\n",
            "        False,  True, False, False, False,  True, False,  True, False, False,\n",
            "         True, False, False, False, False,  True,  True, False,  True, False,\n",
            "         True,  True, False,  True, False, False, False, False, False, False,\n",
            "        False, False,  True, False, False, False,  True,  True, False, False,\n",
            "        False, False,  True, False, False, False, False,  True, False, False,\n",
            "        False, False, False,  True])\n",
            "ntotal: 20\n",
            "crossEntropy: tensor([10.4826, 10.5602, 10.4389, 10.9279, 11.0033, 11.0486, 10.6111, 11.0933,\n",
            "        10.9656, 10.9791, 10.9430, 10.7046, 11.0181, 11.0363, 10.9430, 10.5933,\n",
            "        11.0108, 10.7065, 11.0394, 11.0528, 10.6196, 10.9761, 10.9259, 11.0310,\n",
            "        10.9966, 10.4504, 10.3698, 11.0437, 10.7064, 11.1211, 10.6407, 10.6715,\n",
            "        11.0049, 10.4846, 11.0065, 11.0253, 10.9728, 11.0342, 10.9962, 10.9800,\n",
            "        11.0204, 11.0394, 10.5262, 10.9652, 11.1070, 10.9984, 10.6916, 10.6007,\n",
            "        11.0173, 10.9956, 11.1293, 11.0490, 10.5389, 10.9679, 10.9872, 10.8995,\n",
            "        11.0208, 10.5898, 11.0073, 11.0165, 11.0675, 10.9473, 10.9072, 10.6759],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 10.583163261413574, ntotal item: 20\n",
            "input: tensor([[3.7278e-05, 1.5327e-05, 1.8269e-05,  ..., 1.7556e-05, 1.6837e-05,\n",
            "         1.5182e-05],\n",
            "        [3.4962e-05, 1.5881e-05, 1.7087e-05,  ..., 1.7675e-05, 1.5799e-05,\n",
            "         1.6059e-05],\n",
            "        [3.9499e-05, 1.6490e-05, 1.7636e-05,  ..., 1.6729e-05, 1.5106e-05,\n",
            "         1.6125e-05],\n",
            "        ...,\n",
            "        [2.3546e-05, 1.6774e-05, 1.8271e-05,  ..., 1.7490e-05, 1.5619e-05,\n",
            "         1.6820e-05],\n",
            "        [2.6351e-05, 1.7341e-05, 1.6522e-05,  ..., 1.7670e-05, 1.5332e-05,\n",
            "         1.5549e-05],\n",
            "        [3.1675e-05, 1.6208e-05, 1.7646e-05,  ..., 1.7840e-05, 1.5962e-05,\n",
            "         1.7231e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0, 13301,\n",
            "        31756,     0,   444,     0, 15779,     0,  1724,     0, 22490,     0,\n",
            "            0, 26670,     0, 30484,   250,     0,     0,   126,     0,   115,\n",
            "            0,     0,     0,     0,     0,   767,     0, 59988,     0,  8395,\n",
            "        11686, 50346,     0,     0,     0,     0,     0,     0,  5699,     0,\n",
            "        15389,  3957,     0,  1202,     0,   163,   123,     0,     0,    52,\n",
            "           97,    24,     0,     0]), mask: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
            "        False,  True, False,  True, False,  True, False,  True, False,  True,\n",
            "         True, False,  True, False, False,  True,  True, False,  True, False,\n",
            "         True,  True,  True,  True,  True, False,  True, False,  True, False,\n",
            "        False, False,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "        False, False,  True, False,  True, False, False,  True,  True, False,\n",
            "        False, False,  True,  True])\n",
            "ntotal: 39\n",
            "crossEntropy: tensor([10.1971, 10.2613, 10.1392, 10.8388, 10.6664, 10.4424, 10.2934, 10.4798,\n",
            "        10.4130, 11.0350, 10.9685, 10.3217, 11.0458, 10.4876, 11.0504, 10.3154,\n",
            "        11.0358, 10.4239, 11.0344, 10.5130, 10.2999, 10.9702, 10.5801, 11.0246,\n",
            "        11.1029, 10.1460, 10.0843, 10.9831, 10.3936, 11.0013, 10.3678, 10.2983,\n",
            "        10.6338, 10.1908, 10.5818, 11.0025, 10.6513, 11.0575, 10.6584, 11.0311,\n",
            "        10.9852, 11.0522, 10.2313, 10.7148, 10.6669, 10.6990, 10.3488, 10.2969,\n",
            "        11.1313, 10.5647, 10.9987, 11.0381, 10.2238, 11.0472, 10.6646, 11.0962,\n",
            "        10.9859, 10.3162, 10.5785, 11.0455, 10.9443, 11.0268, 10.5440, 10.3600],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 10.433036804199219, ntotal item: 39\n",
            "input: tensor([[4.7287e-05, 1.5678e-05, 1.8553e-05,  ..., 1.7365e-05, 1.6561e-05,\n",
            "         1.5494e-05],\n",
            "        [4.4471e-05, 1.5426e-05, 1.6848e-05,  ..., 1.7659e-05, 1.5456e-05,\n",
            "         1.6309e-05],\n",
            "        [5.3702e-05, 1.6536e-05, 1.8010e-05,  ..., 1.6808e-05, 1.4582e-05,\n",
            "         1.6819e-05],\n",
            "        ...,\n",
            "        [2.1848e-05, 1.6383e-05, 1.7572e-05,  ..., 1.7575e-05, 1.5673e-05,\n",
            "         1.7492e-05],\n",
            "        [3.4714e-05, 1.7315e-05, 1.6719e-05,  ..., 1.7498e-05, 1.4968e-05,\n",
            "         1.5926e-05],\n",
            "        [4.0035e-05, 1.6511e-05, 1.7734e-05,  ..., 1.7483e-05, 1.5524e-05,\n",
            "         1.7381e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "        37943,     0,   343,     0, 55519,     0,   171,     0,     0,     0,\n",
            "            0, 14833,     0,  1592,     0,     0,     0,     0,     0, 16103,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,  2362,\n",
            "           72,     0,     0,     0,     0,     0,     0,     0, 60338,     0,\n",
            "          274, 11600,     0,    45,     0,  1268,  3012,     0,     0,  6881,\n",
            "          451, 60688,     0,     0]), mask: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False,  True, False,  True, False,  True, False,  True,  True,  True,\n",
            "         True, False,  True, False,  True,  True,  True,  True,  True, False,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True, False,\n",
            "        False,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "        False, False,  True, False,  True, False, False,  True,  True, False,\n",
            "        False, False,  True,  True])\n",
            "ntotal: 46\n",
            "crossEntropy: tensor([ 9.9593, 10.0207,  9.8321, 10.4758, 10.3727, 10.1628,  9.9943, 10.2232,\n",
            "        10.1376, 10.6383, 11.0132, 10.0191, 11.0576, 10.1940, 11.0721, 10.0502,\n",
            "        11.0274, 10.1481, 10.6698, 10.2024, 10.0004, 11.0920, 10.2715, 11.0287,\n",
            "        10.5604,  9.8858,  9.8430, 10.6674, 10.1066, 10.9874, 10.1031,  9.9683,\n",
            "        10.3747,  9.8878, 10.3352, 10.7741, 10.3736, 10.4691, 10.3312, 11.0050,\n",
            "        11.0384, 10.6307, 10.0010, 10.3767, 10.3861, 10.3741, 10.0453,  9.9628,\n",
            "        11.0747, 10.3154, 11.1048, 10.9998,  9.9772, 11.1355, 10.3752, 10.8706,\n",
            "        10.9418, 10.0503, 10.2865, 11.0040, 10.9006, 10.9988, 10.2684, 10.1258],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 10.222343444824219, ntotal item: 46\n",
            "input: tensor([[5.5174e-05, 1.5394e-05, 1.9136e-05,  ..., 1.7657e-05, 1.6416e-05,\n",
            "         1.6365e-05],\n",
            "        [5.3036e-05, 1.5537e-05, 1.6834e-05,  ..., 1.7481e-05, 1.5257e-05,\n",
            "         1.6737e-05],\n",
            "        [6.4729e-05, 1.6882e-05, 1.8422e-05,  ..., 1.6941e-05, 1.4133e-05,\n",
            "         1.7087e-05],\n",
            "        ...,\n",
            "        [2.1726e-05, 1.6494e-05, 1.7433e-05,  ..., 1.7390e-05, 1.5732e-05,\n",
            "         1.7475e-05],\n",
            "        [4.5217e-05, 1.7171e-05, 1.6738e-05,  ..., 1.7279e-05, 1.4749e-05,\n",
            "         1.6282e-05],\n",
            "        [4.9860e-05, 1.6438e-05, 1.8009e-05,  ..., 1.7536e-05, 1.5216e-05,\n",
            "         1.7450e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           24,     0,   242,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0, 30463,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "          861,     0,     0,     0,     0,     0,     0,     0, 42743,     0,\n",
            "            0,     0,     0,     0,     0,  8030,     0,     0,     0,     0,\n",
            "           24, 60689,     0,     0]), mask: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True, False,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
            "        False, False,  True,  True])\n",
            "ntotal: 56\n",
            "crossEntropy: tensor([ 9.8050,  9.8445,  9.6453, 10.1241, 10.0869,  9.8884,  9.7853,  9.9676,\n",
            "         9.8823, 10.3674, 11.0334,  9.8051, 11.0116,  9.9154, 10.5098,  9.8383,\n",
            "        10.7291,  9.9254, 10.3272,  9.9276,  9.8304, 10.6438, 10.0179, 10.9949,\n",
            "        10.2078,  9.7257,  9.7117, 10.3813,  9.9113, 10.8097,  9.9048,  9.8158,\n",
            "        10.1264,  9.7212, 10.0858, 10.4925, 10.1012, 10.2136, 10.0003, 10.8165,\n",
            "        10.9808, 10.3285,  9.8008, 10.0719, 10.1378, 10.1348,  9.8348,  9.7600,\n",
            "        11.1040, 10.0889, 10.5817, 10.8004,  9.8235, 10.7638, 10.0500, 11.1320,\n",
            "        10.7909,  9.8788, 10.0381, 10.7674, 11.0059, 11.0407, 10.0040,  9.9063],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 10.115267753601074, ntotal item: 56\n",
            "input: tensor([[6.1873e-05, 1.5719e-05, 1.9368e-05,  ..., 1.7368e-05, 1.5935e-05,\n",
            "         1.6521e-05],\n",
            "        [5.9497e-05, 1.5512e-05, 1.7123e-05,  ..., 1.7418e-05, 1.5214e-05,\n",
            "         1.6682e-05],\n",
            "        [7.3432e-05, 1.7241e-05, 1.8423e-05,  ..., 1.6928e-05, 1.3892e-05,\n",
            "         1.7219e-05],\n",
            "        ...,\n",
            "        [2.0977e-05, 1.6785e-05, 1.6927e-05,  ..., 1.7604e-05, 1.6182e-05,\n",
            "         1.7398e-05],\n",
            "        [5.2515e-05, 1.6990e-05, 1.6976e-05,  ..., 1.7277e-05, 1.4531e-05,\n",
            "         1.6496e-05],\n",
            "        [5.6634e-05, 1.6491e-05, 1.8217e-05,  ..., 1.7422e-05, 1.4864e-05,\n",
            "         1.7463e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1104,    0,\n",
            "        2352,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0, 3667,    0,    0,    0,    0,    0,    0,    0,\n",
            "        5702,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "         250,    0,    0,    0]), mask: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False,  True,  True,  True])\n",
            "ntotal: 59\n",
            "crossEntropy: tensor([ 9.6904,  9.7296,  9.5191,  9.8734,  9.8694,  9.6379,  9.6446,  9.7762,\n",
            "         9.7112, 10.1029, 10.9899,  9.7175, 11.0140,  9.7239, 10.2699,  9.7528,\n",
            "        10.3632,  9.8304, 10.0248,  9.7620,  9.6523, 10.3098,  9.8109, 10.7354,\n",
            "         9.8678,  9.5880,  9.6346, 10.0955,  9.7433, 10.4978,  9.7675,  9.6772,\n",
            "         9.9406,  9.6044,  9.8855, 10.1996,  9.9163,  9.9291,  9.8122, 10.5459,\n",
            "        11.0076, 10.1024,  9.6871,  9.8447,  9.9477,  9.9215,  9.7468,  9.5924,\n",
            "        11.0003,  9.8943, 10.2209, 10.5029,  9.6994, 10.4481,  9.8165, 10.7402,\n",
            "        10.4775,  9.7871,  9.8784, 10.4480, 11.1072, 10.7721,  9.8544,  9.7789],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 9.955491065979004, ntotal item: 59\n",
            "input: tensor([[6.4100e-05, 1.5671e-05, 1.8956e-05,  ..., 1.7638e-05, 1.5893e-05,\n",
            "         1.6595e-05],\n",
            "        [6.2436e-05, 1.5751e-05, 1.7074e-05,  ..., 1.7231e-05, 1.4659e-05,\n",
            "         1.7189e-05],\n",
            "        [7.9805e-05, 1.7304e-05, 1.8683e-05,  ..., 1.6823e-05, 1.3462e-05,\n",
            "         1.7243e-05],\n",
            "        ...,\n",
            "        [2.8452e-05, 1.6734e-05, 1.7452e-05,  ..., 1.7391e-05, 1.5823e-05,\n",
            "         1.7153e-05],\n",
            "        [5.6016e-05, 1.6915e-05, 1.7481e-05,  ..., 1.7191e-05, 1.4396e-05,\n",
            "         1.6748e-05],\n",
            "        [6.0458e-05, 1.6480e-05, 1.8041e-05,  ..., 1.7278e-05, 1.4320e-05,\n",
            "         1.7805e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0, 17611,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           24,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0]), mask: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True])\n",
            "ntotal: 62\n",
            "crossEntropy: tensor([ 9.6551,  9.6814,  9.4359,  9.7264,  9.7143,  9.4795,  9.5648,  9.6414,\n",
            "         9.6263,  9.9375, 10.7538,  9.6534, 10.9837,  9.6795,  9.9815,  9.7005,\n",
            "         9.9901,  9.7672,  9.8156,  9.6468,  9.4941, 10.0264,  9.6648, 10.4028,\n",
            "         9.6621,  9.5571,  9.6039,  9.9069,  9.6541, 10.1677,  9.7361,  9.6012,\n",
            "         9.8184,  9.5759,  9.7248, 10.0117,  9.8301,  9.7314,  9.7082, 10.2612,\n",
            "        11.0455,  9.8964,  9.6920,  9.7042,  9.8306,  9.7993,  9.6739,  9.4938,\n",
            "        10.7917,  9.7902,  9.9485, 10.2236,  9.6723, 10.2166,  9.7086, 10.4299,\n",
            "        10.1710,  9.7438,  9.7543, 10.1610, 10.6017, 10.4673,  9.7899,  9.7136],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 9.852635383605957, ntotal item: 62\n",
            "input: tensor([[6.6888e-05, 1.5524e-05, 1.9057e-05,  ..., 1.7368e-05, 1.5579e-05,\n",
            "         1.6608e-05],\n",
            "        [6.7795e-05, 1.5644e-05, 1.7095e-05,  ..., 1.7062e-05, 1.4665e-05,\n",
            "         1.7309e-05],\n",
            "        [8.1290e-05, 1.7175e-05, 1.9195e-05,  ..., 1.6688e-05, 1.3349e-05,\n",
            "         1.7456e-05],\n",
            "        ...,\n",
            "        [3.9305e-05, 1.6432e-05, 1.8442e-05,  ..., 1.7372e-05, 1.5109e-05,\n",
            "         1.7057e-05],\n",
            "        [5.6863e-05, 1.6780e-05, 1.7514e-05,  ..., 1.7253e-05, 1.4588e-05,\n",
            "         1.6671e-05],\n",
            "        [6.3000e-05, 1.6725e-05, 1.8285e-05,  ..., 1.7278e-05, 1.4311e-05,\n",
            "         1.8366e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,   236,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "        18767,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0]), mask: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True])\n",
            "ntotal: 62\n",
            "crossEntropy: tensor([ 9.6125,  9.5990,  9.4175,  9.6618,  9.6056,  9.4228,  9.5144,  9.5939,\n",
            "         9.5590,  9.7793, 10.3872,  9.6019, 10.9519,  9.6356,  9.8440,  9.6916,\n",
            "         9.7495,  9.7061,  9.6659,  9.5992,  9.4105,  9.7499,  9.5801, 10.1162,\n",
            "         9.5527,  9.5179,  9.5352,  9.8027,  9.6179,  9.9120,  9.7180,  9.5654,\n",
            "         9.7551,  9.4972,  9.6277,  9.8374,  9.7281,  9.6055,  9.6307, 10.0463,\n",
            "        11.1061,  9.7838,  9.6579,  9.6080,  9.7574,  9.7498,  9.6258,  9.4503,\n",
            "        10.4830,  9.7182,  9.7911, 10.0233,  9.6657, 10.0387,  9.5824, 10.1681,\n",
            "         9.9109,  9.7114,  9.6931,  9.9157, 10.2824, 10.1442,  9.7749,  9.6724],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 9.73647403717041, ntotal item: 62\n",
            "input: tensor([[6.9038e-05, 1.5131e-05, 1.9241e-05,  ..., 1.7457e-05, 1.5522e-05,\n",
            "         1.6534e-05],\n",
            "        [6.8023e-05, 1.5599e-05, 1.7347e-05,  ..., 1.7001e-05, 1.4874e-05,\n",
            "         1.7240e-05],\n",
            "        [8.4220e-05, 1.7239e-05, 1.8874e-05,  ..., 1.6527e-05, 1.3382e-05,\n",
            "         1.7555e-05],\n",
            "        ...,\n",
            "        [4.9082e-05, 1.6393e-05, 1.8560e-05,  ..., 1.7272e-05, 1.4725e-05,\n",
            "         1.7238e-05],\n",
            "        [5.9084e-05, 1.6654e-05, 1.8028e-05,  ..., 1.7348e-05, 1.4541e-05,\n",
            "         1.6522e-05],\n",
            "        [6.4522e-05, 1.6567e-05, 1.8287e-05,  ..., 1.7551e-05, 1.4422e-05,\n",
            "         1.8594e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,   576,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "        10143,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0]), mask: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True])\n",
            "ntotal: 62\n",
            "crossEntropy: tensor([ 9.5809,  9.5957,  9.3821,  9.6240,  9.5748,  9.4293,  9.4732,  9.5633,\n",
            "         9.5273,  9.6930, 10.0319,  9.5277, 10.9748,  9.5001,  9.7442,  9.6326,\n",
            "         9.5566,  9.6920,  9.5727,  9.5340,  9.4589,  9.6206,  9.4933,  9.8574,\n",
            "         9.4625,  9.5219,  9.5043,  9.7494,  9.5744,  9.7423,  9.7095,  9.5226,\n",
            "         9.7210,  9.4603,  9.6226,  9.7477,  9.6679,  9.4889,  9.6105,  9.9165,\n",
            "        10.9893,  9.7084,  9.6763,  9.5636,  9.7132,  9.6976,  9.5844,  9.4465,\n",
            "        10.1627,  9.6520,  9.6501,  9.8661,  9.6690,  9.9276,  9.5069,  9.9421,\n",
            "         9.7870,  9.7118,  9.6366,  9.7486,  9.9998,  9.9220,  9.7366,  9.6485],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 9.65556812286377, ntotal item: 62\n",
            "input: tensor([[7.0630e-05, 1.5340e-05, 1.9222e-05,  ..., 1.7482e-05, 1.5564e-05,\n",
            "         1.6342e-05],\n",
            "        [7.0170e-05, 1.5962e-05, 1.7217e-05,  ..., 1.6751e-05, 1.4691e-05,\n",
            "         1.6840e-05],\n",
            "        [8.3609e-05, 1.6893e-05, 1.8957e-05,  ..., 1.6613e-05, 1.3444e-05,\n",
            "         1.7394e-05],\n",
            "        ...,\n",
            "        [5.7740e-05, 1.6679e-05, 1.8776e-05,  ..., 1.6894e-05, 1.4359e-05,\n",
            "         1.6963e-05],\n",
            "        [6.0295e-05, 1.6965e-05, 1.7994e-05,  ..., 1.7488e-05, 1.4629e-05,\n",
            "         1.6910e-05],\n",
            "        [6.3691e-05, 1.6237e-05, 1.8326e-05,  ..., 1.7460e-05, 1.4441e-05,\n",
            "         1.7870e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 43,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), mask: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True])\n",
            "ntotal: 63\n",
            "crossEntropy: tensor([ 9.5581,  9.5646,  9.3894,  9.6118,  9.5447,  9.3854,  9.4945,  9.5006,\n",
            "         9.4983,  9.6394,  9.8641,  9.5057, 11.0579,  9.4710,  9.6759,  9.6337,\n",
            "         9.4855,  9.6599,  9.5476,  9.4731,  9.3708,  9.5222,  9.4935,  9.7125,\n",
            "         9.3835,  9.5503,  9.5063,  9.7116,  9.6140,  9.6613,  9.6806,  9.5136,\n",
            "         9.7013,  9.4653,  9.5639,  9.7124,  9.6492,  9.4422,  9.6172,  9.8514,\n",
            "        10.7633,  9.6750,  9.6536,  9.5195,  9.6608,  9.6757,  9.6207,  9.4194,\n",
            "         9.9231,  9.6822,  9.6214,  9.7678,  9.6111,  9.8182,  9.4880,  9.8292,\n",
            "         9.6864,  9.6744,  9.6081,  9.6807,  9.7936,  9.7596,  9.7163,  9.6615],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 9.627552032470703, ntotal item: 63\n",
            "input: tensor([[7.0687e-05, 1.5392e-05, 1.9198e-05,  ..., 1.7138e-05, 1.5621e-05,\n",
            "         1.6496e-05],\n",
            "        [7.1156e-05, 1.5886e-05, 1.6671e-05,  ..., 1.6732e-05, 1.4543e-05,\n",
            "         1.7402e-05],\n",
            "        [8.3365e-05, 1.7180e-05, 1.8519e-05,  ..., 1.6486e-05, 1.3437e-05,\n",
            "         1.7435e-05],\n",
            "        ...,\n",
            "        [5.8987e-05, 1.6621e-05, 1.8467e-05,  ..., 1.6947e-05, 1.4470e-05,\n",
            "         1.7566e-05],\n",
            "        [6.1598e-05, 1.6626e-05, 1.8009e-05,  ..., 1.7379e-05, 1.4408e-05,\n",
            "         1.6788e-05],\n",
            "        [6.2999e-05, 1.6290e-05, 1.8355e-05,  ..., 1.7459e-05, 1.4324e-05,\n",
            "         1.7902e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0, 13760,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0]), mask: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True])\n",
            "ntotal: 63\n",
            "crossEntropy: tensor([ 9.5572,  9.5506,  9.3923,  9.6214,  9.4922,  9.3728,  9.4660,  9.4896,\n",
            "         9.4946,  9.6063,  9.7688,  9.4721, 11.1205,  9.3723,  9.6462,  9.6097,\n",
            "         9.4275,  9.6701,  9.5491,  9.5018,  9.4199,  9.4696,  9.4651,  9.6227,\n",
            "         9.3727,  9.5514,  9.4844,  9.6606,  9.5924,  9.5959,  9.6702,  9.4940,\n",
            "         9.6872,  9.4623,  9.5615,  9.6563,  9.6541,  9.4279,  9.6008,  9.8127,\n",
            "        10.4767,  9.6437,  9.6605,  9.5144,  9.6757,  9.6544,  9.6015,  9.4308,\n",
            "         9.7817,  9.6452,  9.5611,  9.7425,  9.6077,  9.7408,  9.4951,  9.7855,\n",
            "         9.6177,  9.6708,  9.5804,  9.6452,  9.6682,  9.7382,  9.6949,  9.6724],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 9.597264289855957, ntotal item: 63\n",
            "input: tensor([[7.1690e-05, 1.5186e-05, 1.9458e-05,  ..., 1.7131e-05, 1.5594e-05,\n",
            "         1.6935e-05],\n",
            "        [7.1110e-05, 1.5591e-05, 1.6937e-05,  ..., 1.6636e-05, 1.4396e-05,\n",
            "         1.7417e-05],\n",
            "        [8.3362e-05, 1.6972e-05, 1.8392e-05,  ..., 1.6793e-05, 1.3739e-05,\n",
            "         1.7660e-05],\n",
            "        ...,\n",
            "        [6.2806e-05, 1.6707e-05, 1.8447e-05,  ..., 1.7110e-05, 1.4508e-05,\n",
            "         1.7580e-05],\n",
            "        [6.1227e-05, 1.6490e-05, 1.8218e-05,  ..., 1.6938e-05, 1.4432e-05,\n",
            "         1.7052e-05],\n",
            "        [6.4932e-05, 1.6294e-05, 1.8682e-05,  ..., 1.7272e-05, 1.4017e-05,\n",
            "         1.7997e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 52,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), mask: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True])\n",
            "ntotal: 63\n",
            "crossEntropy: tensor([ 9.5432,  9.5513,  9.3923,  9.6036,  9.5063,  9.4093,  9.5120,  9.4597,\n",
            "         9.4543,  9.6072,  9.6525,  9.5222, 11.0386,  9.3666,  9.5655,  9.6035,\n",
            "         9.4306,  9.7117,  9.5390,  9.4942,  9.4080,  9.4374,  9.4203,  9.5781,\n",
            "         9.3697,  9.5278,  9.4798,  9.6109,  9.6415,  9.5794,  9.6646,  9.5508,\n",
            "         9.6451,  9.4610,  9.5686,  9.6675,  9.6587,  9.4400,  9.5881,  9.7681,\n",
            "        10.1726,  9.6188,  9.6172,  9.5283,  9.6889,  9.6357,  9.5962,  9.4406,\n",
            "         9.7051,  9.6253,  9.5069,  9.7426,  9.6148,  9.6813,  9.4820,  9.7788,\n",
            "         9.6045,  9.7002,  9.5956,  9.6046,  9.5813,  9.6755,  9.7009,  9.6422],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 9.579851150512695, ntotal item: 63\n",
            "input: tensor([[7.0271e-05, 1.5227e-05, 1.9306e-05,  ..., 1.7632e-05, 1.5467e-05,\n",
            "         1.6745e-05],\n",
            "        [6.9481e-05, 1.5472e-05, 1.7070e-05,  ..., 1.6928e-05, 1.4257e-05,\n",
            "         1.7069e-05],\n",
            "        [7.9809e-05, 1.6729e-05, 1.8468e-05,  ..., 1.6800e-05, 1.3844e-05,\n",
            "         1.7672e-05],\n",
            "        ...,\n",
            "        [6.6263e-05, 1.6301e-05, 1.8485e-05,  ..., 1.7070e-05, 1.4148e-05,\n",
            "         1.7482e-05],\n",
            "        [6.0992e-05, 1.6516e-05, 1.8262e-05,  ..., 1.6982e-05, 1.4409e-05,\n",
            "         1.7186e-05],\n",
            "        [6.3555e-05, 1.6299e-05, 1.8494e-05,  ..., 1.7310e-05, 1.4362e-05,\n",
            "         1.7931e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "        4061,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0]), mask: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True])\n",
            "ntotal: 63\n",
            "crossEntropy: tensor([ 9.5631,  9.5745,  9.4359,  9.5893,  9.5142,  9.3881,  9.5377,  9.4404,\n",
            "         9.4540,  9.5838,  9.5974,  9.5306, 11.1154,  9.5293,  9.5495,  9.6313,\n",
            "         9.4144,  9.7038,  9.4956,  9.4880,  9.4572,  9.4024,  9.4444,  9.5873,\n",
            "         9.3511,  9.4901,  9.5176,  9.6133,  9.6304,  9.6004,  9.6828,  9.5074,\n",
            "         9.6065,  9.4866,  9.5687,  9.6528,  9.6516,  9.4644,  9.5680,  9.7494,\n",
            "         9.9673,  9.6472,  9.5987,  9.4986,  9.6825,  9.6372,  9.5867,  9.4525,\n",
            "         9.6952,  9.5915,  9.4611,  9.7241,  9.5976,  9.6727,  9.4779,  9.7717,\n",
            "         9.5624,  9.6540,  9.6092,  9.5762,  9.5710,  9.6219,  9.7048,  9.6636],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 9.572676658630371, ntotal item: 63\n",
            "input: tensor([[7.2632e-05, 1.5367e-05, 1.9126e-05,  ..., 1.7224e-05, 1.5522e-05,\n",
            "         1.6891e-05],\n",
            "        [7.0858e-05, 1.5287e-05, 1.7443e-05,  ..., 1.6768e-05, 1.4088e-05,\n",
            "         1.7035e-05],\n",
            "        [7.8790e-05, 1.6709e-05, 1.8115e-05,  ..., 1.6798e-05, 1.3829e-05,\n",
            "         1.7529e-05],\n",
            "        ...,\n",
            "        [6.8466e-05, 1.6692e-05, 1.8529e-05,  ..., 1.7132e-05, 1.4350e-05,\n",
            "         1.7090e-05],\n",
            "        [6.3046e-05, 1.6472e-05, 1.8238e-05,  ..., 1.7007e-05, 1.4470e-05,\n",
            "         1.7249e-05],\n",
            "        [6.3431e-05, 1.6631e-05, 1.8527e-05,  ..., 1.7630e-05, 1.4169e-05,\n",
            "         1.8054e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "        4877,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0]), mask: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True])\n",
            "ntotal: 63\n",
            "crossEntropy: tensor([ 9.5301,  9.5548,  9.4487,  9.5665,  9.5129,  9.4660,  9.5078,  9.4420,\n",
            "         9.4281,  9.5766,  9.5792,  9.5025, 10.8640,  9.4815,  9.5460,  9.6337,\n",
            "         9.3716,  9.7175,  9.4932,  9.4669,  9.4802,  9.4300,  9.4464,  9.5434,\n",
            "         9.3826,  9.5190,  9.5462,  9.6275,  9.6362,  9.5502,  9.6794,  9.5343,\n",
            "         9.6231,  9.4632,  9.5930,  9.6659,  9.6604,  9.4232,  9.5602,  9.7360,\n",
            "         9.8405,  9.6524,  9.6153,  9.4914,  9.6701,  9.6894,  9.5894,  9.4132,\n",
            "         9.6061,  9.5876,  9.5155,  9.6742,  9.6067,  9.6767,  9.4891,  9.7385,\n",
            "         9.5538,  9.6654,  9.6452,  9.5926,  9.5113,  9.5892,  9.6716,  9.6656],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 9.566292762756348, ntotal item: 63\n",
            "input: tensor([[6.8455e-05, 1.5228e-05, 1.9421e-05,  ..., 1.7152e-05, 1.5507e-05,\n",
            "         1.6453e-05],\n",
            "        [6.8015e-05, 1.5351e-05, 1.7336e-05,  ..., 1.6885e-05, 1.4330e-05,\n",
            "         1.7428e-05],\n",
            "        [8.0371e-05, 1.7000e-05, 1.8192e-05,  ..., 1.6478e-05, 1.3975e-05,\n",
            "         1.7607e-05],\n",
            "        ...,\n",
            "        [6.7037e-05, 1.6342e-05, 1.8724e-05,  ..., 1.6954e-05, 1.3816e-05,\n",
            "         1.7242e-05],\n",
            "        [6.2410e-05, 1.6379e-05, 1.8214e-05,  ..., 1.7224e-05, 1.4539e-05,\n",
            "         1.6768e-05],\n",
            "        [6.4336e-05, 1.6545e-05, 1.8404e-05,  ..., 1.7777e-05, 1.4003e-05,\n",
            "         1.8178e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 350,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0]), mask: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True])\n",
            "ntotal: 63\n",
            "crossEntropy: tensor([ 9.5893,  9.5958,  9.4289,  9.5646,  9.4975,  9.3773,  9.4929,  9.4751,\n",
            "         9.4599,  9.5662,  9.5243,  9.4906, 10.9057,  9.5693,  9.5881,  9.6079,\n",
            "         9.3548,  9.7018,  9.4562,  9.5064,  9.4748,  9.4111,  9.4458,  9.5222,\n",
            "         9.3554,  9.5527,  9.5570,  9.6142,  9.6440,  9.5462,  9.6445,  9.5438,\n",
            "         9.6555,  9.4441,  9.5661,  9.6466,  9.6678,  9.4420,  9.5666,  9.7162,\n",
            "         9.7727,  9.6423,  9.6371,  9.4679,  9.6611,  9.6513,  9.5555,  9.4268,\n",
            "         9.5959,  9.5828,  9.4847,  9.6845,  9.5952,  9.6498,  9.4877,  9.7106,\n",
            "         9.5481,  9.6703,  9.6235,  9.6295,  9.4904,  9.6103,  9.6818,  9.6514],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 9.561498641967773, ntotal item: 63\n",
            "input: tensor([[7.0006e-05, 1.5166e-05, 1.9313e-05,  ..., 1.7506e-05, 1.5435e-05,\n",
            "         1.6326e-05],\n",
            "        [6.8111e-05, 1.5486e-05, 1.7150e-05,  ..., 1.6743e-05, 1.4305e-05,\n",
            "         1.7018e-05],\n",
            "        [7.9898e-05, 1.7145e-05, 1.7817e-05,  ..., 1.6742e-05, 1.4229e-05,\n",
            "         1.7507e-05],\n",
            "        ...,\n",
            "        [6.6958e-05, 1.6159e-05, 1.8816e-05,  ..., 1.7170e-05, 1.3908e-05,\n",
            "         1.7189e-05],\n",
            "        [6.3214e-05, 1.6328e-05, 1.8192e-05,  ..., 1.7738e-05, 1.4429e-05,\n",
            "         1.6870e-05],\n",
            "        [6.3787e-05, 1.6312e-05, 1.8222e-05,  ..., 1.7798e-05, 1.3922e-05,\n",
            "         1.8250e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 26,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), mask: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True])\n",
            "ntotal: 63\n",
            "crossEntropy: tensor([ 9.5669,  9.5944,  9.4348,  9.5386,  9.5024,  9.3991,  9.4707,  9.4518,\n",
            "         9.4278,  9.5535,  9.5143,  9.5161, 11.0471,  9.5133,  9.6084,  9.6225,\n",
            "         9.3774,  9.6932,  9.4777,  9.5310,  9.4129,  9.4389,  9.4459,  9.5232,\n",
            "         9.3728,  9.5288,  9.5308,  9.6020,  9.6411,  9.4939,  9.6486,  9.5501,\n",
            "         9.6594,  9.4533,  9.6032,  9.6253,  9.6850,  9.4521,  9.5472,  9.6933,\n",
            "         9.7156,  9.6216,  9.6430,  9.5021,  9.6444,  9.6473,  9.5650,  9.4108,\n",
            "         9.5907,  9.6106,  9.5046,  9.6430,  9.5739,  9.6494,  9.5093,  9.7238,\n",
            "         9.5483,  9.6362,  9.5798,  9.6079,  9.4911,  9.6114,  9.6690,  9.6600],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 9.556514739990234, ntotal item: 63\n",
            "input: tensor([[7.1693e-05, 1.5231e-05, 1.9175e-05,  ..., 1.7358e-05, 1.5471e-05,\n",
            "         1.6206e-05],\n",
            "        [6.6176e-05, 1.5644e-05, 1.6969e-05,  ..., 1.6890e-05, 1.4136e-05,\n",
            "         1.7112e-05],\n",
            "        [8.2824e-05, 1.7101e-05, 1.7845e-05,  ..., 1.6509e-05, 1.3931e-05,\n",
            "         1.7299e-05],\n",
            "        ...,\n",
            "        [6.6383e-05, 1.6091e-05, 1.8367e-05,  ..., 1.7429e-05, 1.4172e-05,\n",
            "         1.7399e-05],\n",
            "        [6.3977e-05, 1.6246e-05, 1.8356e-05,  ..., 1.7733e-05, 1.4159e-05,\n",
            "         1.6691e-05],\n",
            "        [6.3454e-05, 1.6258e-05, 1.8296e-05,  ..., 1.7397e-05, 1.3809e-05,\n",
            "         1.8311e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "        1399,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0]), mask: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True])\n",
            "ntotal: 63\n",
            "crossEntropy: tensor([ 9.5431,  9.6232,  9.3988,  9.5651,  9.5165,  9.3707,  9.4575,  9.4728,\n",
            "         9.4367,  9.5742,  9.5587,  9.4628, 11.0001,  9.5656,  9.6281,  9.6279,\n",
            "         9.3923,  9.6611,  9.4731,  9.5031,  9.4476,  9.4516,  9.4295,  9.5419,\n",
            "         9.3548,  9.5821,  9.5506,  9.5894,  9.6519,  9.4375,  9.6643,  9.5245,\n",
            "         9.6552,  9.5206,  9.6021,  9.6117,  9.6711,  9.4126,  9.5656,  9.6921,\n",
            "         9.6658,  9.6302,  9.6569,  9.4970,  9.6536,  9.6426,  9.5212,  9.3981,\n",
            "         9.6056,  9.5777,  9.5333,  9.6054,  9.6018,  9.6697,  9.5295,  9.7033,\n",
            "         9.5643,  9.6646,  9.5695,  9.5750,  9.4800,  9.6201,  9.6570,  9.6652],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 9.556756019592285, ntotal item: 63\n",
            "input: tensor([[7.3039e-05, 1.4569e-05, 1.9434e-05,  ..., 1.7293e-05, 1.5555e-05,\n",
            "         1.6386e-05],\n",
            "        [6.6723e-05, 1.5815e-05, 1.7032e-05,  ..., 1.6882e-05, 1.4395e-05,\n",
            "         1.6988e-05],\n",
            "        [8.1711e-05, 1.7185e-05, 1.8155e-05,  ..., 1.6470e-05, 1.4017e-05,\n",
            "         1.7625e-05],\n",
            "        ...,\n",
            "        [6.8512e-05, 1.6204e-05, 1.8702e-05,  ..., 1.6980e-05, 1.3863e-05,\n",
            "         1.7139e-05],\n",
            "        [6.3808e-05, 1.6357e-05, 1.8029e-05,  ..., 1.7657e-05, 1.3939e-05,\n",
            "         1.6757e-05],\n",
            "        [6.2686e-05, 1.6229e-05, 1.8168e-05,  ..., 1.7245e-05, 1.3972e-05,\n",
            "         1.8343e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 69,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), mask: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True])\n",
            "ntotal: 63\n",
            "crossEntropy: tensor([ 9.5245,  9.6150,  9.4123,  9.5220,  9.5084,  9.4091,  9.4652,  9.4783,\n",
            "         9.4427,  9.5418,  9.5292,  9.4700, 11.0177,  9.5457,  9.6212,  9.6203,\n",
            "         9.3767,  9.6665,  9.4420,  9.5106,  9.4061,  9.4630,  9.4285,  9.5174,\n",
            "         9.3356,  9.5560,  9.5493,  9.6053,  9.6476,  9.5043,  9.6774,  9.5106,\n",
            "         9.6540,  9.5224,  9.5667,  9.6085,  9.6602,  9.4124,  9.5531,  9.6935,\n",
            "         9.6597,  9.6038,  9.6224,  9.4897,  9.6437,  9.6449,  9.5273,  9.4082,\n",
            "         9.6030,  9.6086,  9.5644,  9.6109,  9.6300,  9.6780,  9.4985,  9.6936,\n",
            "         9.5474,  9.6711,  9.5789,  9.5955,  9.4575,  9.5885,  9.6596,  9.6774],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 9.552955627441406, ntotal item: 63\n",
            "input: tensor([[7.1820e-05, 1.4665e-05, 1.9526e-05,  ..., 1.7841e-05, 1.5581e-05,\n",
            "         1.6637e-05],\n",
            "        [6.9228e-05, 1.5608e-05, 1.7191e-05,  ..., 1.7081e-05, 1.4322e-05,\n",
            "         1.6998e-05],\n",
            "        [8.2859e-05, 1.6995e-05, 1.8128e-05,  ..., 1.6496e-05, 1.4059e-05,\n",
            "         1.7777e-05],\n",
            "        ...,\n",
            "        [6.8528e-05, 1.5947e-05, 1.8743e-05,  ..., 1.6964e-05, 1.3715e-05,\n",
            "         1.6841e-05],\n",
            "        [6.1047e-05, 1.6141e-05, 1.7881e-05,  ..., 1.7695e-05, 1.4494e-05,\n",
            "         1.6919e-05],\n",
            "        [6.1425e-05, 1.6316e-05, 1.8012e-05,  ..., 1.6973e-05, 1.4256e-05,\n",
            "         1.8395e-05]], grad_fn=<SoftmaxBackward0>), target: tensor([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 625,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0]), mask: tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "         True,  True,  True,  True])\n",
            "ntotal: 63\n",
            "crossEntropy: tensor([ 9.5414,  9.5781,  9.3984,  9.5198,  9.5104,  9.3738,  9.4621,  9.4530,\n",
            "         9.4469,  9.5789,  9.4939,  9.4954, 11.0068,  9.5715,  9.5995,  9.6069,\n",
            "         9.3352,  9.6625,  9.4879,  9.5065,  9.3913,  9.4079,  9.3926,  9.5011,\n",
            "         9.3416,  9.5977,  9.6034,  9.6317,  9.6375,  9.5207,  9.6835,  9.4998,\n",
            "         9.6338,  9.4912,  9.5393,  9.6250,  9.6756,  9.3870,  9.6148,  9.6931,\n",
            "         9.6671,  9.6286,  9.6094,  9.4920,  9.6436,  9.6539,  9.5648,  9.4383,\n",
            "         9.6202,  9.5670,  9.6095,  9.6000,  9.6183,  9.6821,  9.5005,  9.6796,\n",
            "         9.5478,  9.6885,  9.5910,  9.6199,  9.4365,  9.5883,  9.7039,  9.6977],\n",
            "       grad_fn=<NegBackward0>)\n",
            "loss: 9.554587364196777, ntotal item: 63\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[150], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Run training iterations\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting Training!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m trainIters(model_name, squad_vocab, squad_pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n\u001b[0;32m     38\u001b[0m            embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n\u001b[0;32m     39\u001b[0m            print_every, save_every, clip, corpus_name, loadFilename)\n",
            "Cell \u001b[1;32mIn[149], line 22\u001b[0m, in \u001b[0;36mtrainIters\u001b[1;34m(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename)\u001b[0m\n\u001b[0;32m     19\u001b[0m (input_variable, lengths), (target_variable, mask, max_target_len) \u001b[38;5;241m=\u001b[39m training_batch\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Run a training iteration with batch\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m loss \u001b[38;5;241m=\u001b[39m train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n\u001b[0;32m     23\u001b[0m              decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n\u001b[0;32m     24\u001b[0m print_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Print progress\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[146], line 62\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip, max_length)\u001b[0m\n\u001b[0;32m     59\u001b[0m         n_totals \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m nTotal\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Perform backpropagation\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Clip gradients: gradients are modified in place\u001b[39;00m\n\u001b[0;32m     65\u001b[0m _ \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(encoder\u001b[38;5;241m.\u001b[39mparameters(), clip)\n",
            "File \u001b[1;32mc:\\Users\\rohit\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\rohit\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 4000\n",
        "print_every = 1\n",
        "save_every = 500\n",
        "corpus_name = \"squad_corpus\"\n",
        "save_dir = './checkpoints/'\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "if loadFilename:\n",
        "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "# If you have CUDA, configure CUDA to call\n",
        "for state in encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "for state in decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "# Run training iterations\n",
        "print(\"Starting Training!\")\n",
        "trainIters(model_name, squad_vocab, squad_pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_num_layers, decoder_num_layers, save_dir, n_iteration, batch_size,\n",
        "           print_every, save_every, clip, corpus_name, loadFilename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        # Forward input through encoder model\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "        # Initialize decoder input with SOS_token\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * BOS_token\n",
        "        # Initialize tensors to append decoded words to\n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "        # Iteratively decode one word token at a time\n",
        "        for _ in range(max_length):\n",
        "            # Forward pass through decoder\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # Obtain most likely word token and its softmax score\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            # Record token and score\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            # Prepare current token to be next decoder input (add a dimension)\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "        # Return collections of word tokens and scores\n",
        "        return all_tokens, all_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=22):\n",
        "    ### Format input sentence as a batch\n",
        "    # words -> indexes\n",
        "    indexes_batch = [index_from_sentence(voc, sentence)]\n",
        "    # Create lengths tensor\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    # Transpose dimensions of batch to match models' expectations\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    # Use appropriate device\n",
        "    input_batch = input_batch.to(device)\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "    # Decode sentence with searcher\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "    # indexes -> words\n",
        "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    return decoded_words\n",
        "\n",
        "\n",
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "    input_sentence = ''\n",
        "    while(1):\n",
        "        try:\n",
        "            # Get input sentence\n",
        "            input_sentence = input('> ')\n",
        "            # Check if it is quit case\n",
        "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "            # Normalize sentence\n",
        "            # input_sentence = normalizeString(input_sentence)\n",
        "            # Evaluate sentence\n",
        "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "            # Format and print response sentence\n",
        "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "            print('Bot:', ' '.join(output_words))\n",
        "\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered unknown word.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set dropout layers to ``eval`` mode\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# Initialize search module\n",
        "searcher = GreedySearchDecoder(encoder, decoder)\n",
        "\n",
        "# Begin chatting (uncomment and run the following line to begin)\n",
        "# evaluateInput(encoder, decoder, searcher, voc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'DecoderRNN' object has no attribute 'n_layers'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[155], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m evaluateInput(encoder, decoder, searcher, squad_vocab)\n",
            "Cell \u001b[1;32mIn[153], line 30\u001b[0m, in \u001b[0;36mevaluateInput\u001b[1;34m(encoder, decoder, searcher, voc)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_sentence \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m input_sentence \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Normalize sentence\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# input_sentence = normalizeString(input_sentence)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Evaluate sentence\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m output_words \u001b[38;5;241m=\u001b[39m evaluate(encoder, decoder, searcher, voc, input_sentence)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Format and print response sentence\u001b[39;00m\n\u001b[0;32m     32\u001b[0m output_words[:] \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (x \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEOS\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m x \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPAD\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
            "Cell \u001b[1;32mIn[153], line 13\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(encoder, decoder, searcher, voc, sentence, max_length)\u001b[0m\n\u001b[0;32m     11\u001b[0m lengths \u001b[38;5;241m=\u001b[39m lengths\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Decode sentence with searcher\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m tokens, scores \u001b[38;5;241m=\u001b[39m searcher(input_batch, lengths, max_length)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# indexes -> words\u001b[39;00m\n\u001b[0;32m     15\u001b[0m decoded_words \u001b[38;5;241m=\u001b[39m [voc\u001b[38;5;241m.\u001b[39mindex2word[token\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n",
            "File \u001b[1;32mc:\\Users\\rohit\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\rohit\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[151], line 11\u001b[0m, in \u001b[0;36mGreedySearchDecoder.forward\u001b[1;34m(self, input_seq, input_length, max_length)\u001b[0m\n\u001b[0;32m      9\u001b[0m encoder_outputs, encoder_hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(input_seq, input_length)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Prepare encoder's final hidden layer to be first hidden input to the decoder\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m decoder_hidden \u001b[38;5;241m=\u001b[39m encoder_hidden[:decoder\u001b[38;5;241m.\u001b[39mn_layers]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Initialize decoder input with SOS_token\u001b[39;00m\n\u001b[0;32m     13\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;241m*\u001b[39m BOS_token\n",
            "File \u001b[1;32mc:\\Users\\rohit\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'DecoderRNN' object has no attribute 'n_layers'"
          ]
        }
      ],
      "source": [
        "evaluateInput(encoder, decoder, searcher, squad_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
