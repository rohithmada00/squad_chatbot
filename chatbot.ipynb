{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-bQotCCow75",
        "outputId": "0ee74fb2-b342-4e02-c9d1-f1aa0904d3b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in c:\\users\\rohit\\anaconda3\\lib\\site-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (1.10.8)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
            "Requirement already satisfied: wget in c:\\users\\rohit\\anaconda3\\lib\\site-packages (3.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "meOW4ABlCLUL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import json\n",
        "import tqdm\n",
        "from spacy.lang.en import English\n",
        "import os\n",
        "import random\n",
        "import wget\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "KNz20a0i0lO-"
      },
      "outputs": [],
      "source": [
        "train_path = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json'\n",
        "test_path = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "f0mU32C7o9qK"
      },
      "outputs": [],
      "source": [
        "tokenizer = English()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJlN-Ujo8oNK",
        "outputId": "96c0ea8e-13a3-47e4-d28f-72bb7efd4a9c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'train-v2.0 (1).json'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# !wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
        "\n",
        "wget.download(train_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "vRbvlGd895-M"
      },
      "outputs": [],
      "source": [
        "def open_file(file):\n",
        "  with open(file, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "  for line in lines[:10]:\n",
        "    print(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "mhKarKjAC6po"
      },
      "outputs": [],
      "source": [
        "# open_file('train-v2.0.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "vIT-zv9wDFnm"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json('train-v2.0.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "z-Vkngjtnyc9",
        "outputId": "f1218dd0-5895-4e1e-95ea-be885595b904"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>version</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Beyoncé', 'paragraphs': [{'qas': [{...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Frédéric_Chopin', 'paragraphs': [{'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'Sino-Tibetan_relations_during_the_M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'IPod', 'paragraphs': [{'qas': [{'qu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>v2.0</td>\n",
              "      <td>{'title': 'The_Legend_of_Zelda:_Twilight_Princ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  version                                               data\n",
              "0    v2.0  {'title': 'Beyoncé', 'paragraphs': [{'qas': [{...\n",
              "1    v2.0  {'title': 'Frédéric_Chopin', 'paragraphs': [{'...\n",
              "2    v2.0  {'title': 'Sino-Tibetan_relations_during_the_M...\n",
              "3    v2.0  {'title': 'IPod', 'paragraphs': [{'qas': [{'qu...\n",
              "4    v2.0  {'title': 'The_Legend_of_Zelda:_Twilight_Princ..."
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGZaAeD_Duoc",
        "outputId": "b7c0d2d7-93bf-437a-ce13-a14c3039d58e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "version    object\n",
              "data       object\n",
              "dtype: object"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pXiZKopoqIGI",
        "outputId": "d34e3d47-f738-475c-9fac-708fc5ef2415"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'r'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['data'][0]['paragraphs'][0]['qas'][0]['question'][-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "PMJfr1v4pzDp",
        "outputId": "c1d244dc-a11f-4268-a807-075d340c3ff3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'popular'"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(df['data'][0]['paragraphs'][0]['qas'][0]['question'])[-2].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0y0nff6lnLVq",
        "outputId": "a87ed23e-4c64-41db-ad69-024ee00131ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "442"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "C2g9o4L0rAsC"
      },
      "outputs": [],
      "source": [
        "# tokenize a input sentence into list of words\n",
        "def tokenize_word(sentence):\n",
        "  return [token.text for token in tokenizer(sentence)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "SXOYeqx3nOBh"
      },
      "outputs": [],
      "source": [
        "# clean the input text for the future processing\n",
        "\n",
        "def clean_text(text):\n",
        "  text = text.replace(\"]\", \" ] \")\n",
        "  text = text.replace(\"[\", \" [ \")\n",
        "  text = text.replace(\"\\n\", \" \")\n",
        "  text = text.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "wc1Up9WYrtfs"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "\n",
        "class SquadDataset(data.Dataset):\n",
        "  \"\"\"\n",
        "    Customizing squad dataset to include the following fields for each instance:\n",
        "    1. question - string describing the question\n",
        "    2. answer - string describing the corresponding answer\n",
        "    3. context - the relevent context of question and answer, not all question answer pairs have the same context\n",
        "    4. label - a higher level context, multiple question answer pairs might have same label but different context\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, question, answer, context, label):\n",
        "    self.question = question\n",
        "    self.answer = answer\n",
        "    self.context = context\n",
        "    self.label = label\n",
        "\n",
        "  def __get_item__(self, index):\n",
        "    return self.question[index], self.answer[index], self.context[index], self.label[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "xEFVma9xSPb4"
      },
      "outputs": [],
      "source": [
        "class PreprocessDataset():\n",
        "  def __init__(self, data_dir, tokenizer, max_len = 20):\n",
        "    self.data_dir = data_dir\n",
        "    self.embedding_dim = 10\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def load_data(self, file_name):\n",
        "    print(f'Loading file {file_name}...')\n",
        "    with open(file_name, 'r') as f:\n",
        "      self.data = json.load(f)\n",
        "\n",
        "  def separate_data(self, file_name):\n",
        "    self.load_data(file_name)\n",
        "    sub_dir = 'separate_data'\n",
        "\n",
        "    if not os.path.exists(os.path.join(self.data_dir, sub_dir)):\n",
        "      os.makedirs(os.path.join(self.data_dir, sub_dir))\n",
        "\n",
        "    # create a sub directory to store fields into separate files\n",
        "    with open(os.path.join(self.data_dir, sub_dir, sub_dir + '.context'), 'w', encoding=\"utf-8\") as context_file,\\\n",
        "             open(os.path.join(self.data_dir, sub_dir, sub_dir + '.question'), 'w', encoding=\"utf-8\") as question_file,\\\n",
        "             open(os.path.join(self.data_dir, sub_dir, sub_dir + '.answer'), 'w', encoding=\"utf-8\") as answer_file,\\\n",
        "             open(os.path.join(self.data_dir, sub_dir, sub_dir + '.label'), 'w', encoding=\"utf-8\") as label_file:\n",
        "      for i in tqdm.tqdm(range(len(self.data['data']))):\n",
        "        label = self.data['data'][i]['title']\n",
        "        label_token = self.tokenizer(label)\n",
        "        paragraphs = self.data['data'][i]['paragraphs']\n",
        "\n",
        "        for paragraph in paragraphs:\n",
        "          context = paragraph['context']\n",
        "          # clean context\n",
        "          context_tokens = self.tokenizer(context)\n",
        "\n",
        "          qas = paragraph['qas']\n",
        "          for qa in qas:\n",
        "            # clean qa\n",
        "            question = qa['question']\n",
        "            question_tokens = self.tokenizer(question)\n",
        "            # skip if no answers are found\n",
        "            if(qa['is_impossible'] is True):\n",
        "              continue\n",
        "            # selecting only one answer for now\n",
        "            answer = qa['answers'][0]['text']\n",
        "            answer_tokens = self.tokenizer(answer)\n",
        "\n",
        "            if(len(question_tokens) <= self.max_len and len(answer_tokens) <= self.max_len):\n",
        "              context_file.write(' '.join([token.text for token in context_tokens]) + '\\n')\n",
        "              question_file.write(' '.join([token.text for token in question_tokens]) + '\\n')\n",
        "              answer_file.write(' '.join([token.text for token in answer_tokens]) + '\\n')\n",
        "              label_file.write(''.join(label_token.text) + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwtGWwbF7BlM",
        "outputId": "96e4ae39-88db-43df-d1c0-6727f147ab95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading file train-v2.0.json...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 442/442 [00:23<00:00, 18.64it/s]\n"
          ]
        }
      ],
      "source": [
        "preprocessor = PreprocessDataset('./', tokenizer)\n",
        "preprocessor.separate_data('train-v2.0.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "# vocabulary building\n",
        "\n",
        "# giving out a unique index for these token tags\n",
        "PAD_token = 0\n",
        "BOS_token = 1\n",
        "EOS_token = 2\n",
        "UNK_token = 3\n",
        "\n",
        "PAD_tag = '<PAD>'\n",
        "BOS_tag = '<BOS>'\n",
        "EOS_tag = '<EOS>'\n",
        "UNK_tag = '<UNK>'\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, vocab_name):\n",
        "        self.vocab_name = vocab_name\n",
        "        self.trimmed = False\n",
        "        self.word2idx = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {\n",
        "            PAD_token : PAD_tag,\n",
        "            BOS_token : BOS_tag,\n",
        "            EOS_token : EOS_tag,\n",
        "            UNK_token : UNK_tag,\n",
        "        }\n",
        "        self.total_word_count = 4\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            # adding a new word to vocabulary\n",
        "            self.word2idx[word] = self.total_word_count\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.total_word_count] = word\n",
        "            self.total_word_count += 1\n",
        "        else:\n",
        "            # just increase the word count\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def trim_words(self, min_frequency):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        retain_words = []\n",
        "        for word, frequency in self.word2count.items():\n",
        "            if frequency >= min_frequency:\n",
        "                retain_words.append(word)\n",
        "\n",
        "        # re initialize data\n",
        "        self.word2idx = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {\n",
        "            PAD_token : PAD_tag,\n",
        "            BOS_token : BOS_tag,\n",
        "            EOS_token : EOS_tag,\n",
        "            UNK_token : UNK_tag,\n",
        "        }\n",
        "        self.total_word_count = 4\n",
        "\n",
        "        for word in retain_words:\n",
        "            self.addWord(word)     \n",
        "\n",
        "    def count_high_frequency_words(self, min_frequency):\n",
        "        count = 0\n",
        "        for frequency in self.word2count.values():\n",
        "            if frequency >= min_frequency:\n",
        "                count += 1\n",
        "        return count   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_vocab(vocab_name, file_paths, min_frequency=-1):\n",
        "    vocab = Vocabulary(vocab_name)\n",
        "    for file_path in file_paths:\n",
        "        sentences = open(file_path, encoding='utf-8').read().strip().split('\\n')\n",
        "        for sentence in sentences:\n",
        "            vocab.addSentence(sentence)\n",
        "        if min_frequency != -1:\n",
        "            vocab.trim_words(min_frequency)\n",
        "    return vocab\n",
        "\n",
        "def make_pairs(questions_path, answers_path):\n",
        "    pairs = []\n",
        "    with open(questions_path, encoding='utf-8').read().strip().split('\\n') as questions, \\\n",
        "        open(answers_path, encoding='utf-8').read().strip().split('\\n') as answers:\n",
        "        for i in range(len(questions)):\n",
        "            pairs.append(questions[i], answers[i])\n",
        "    return pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'list' object does not support the context manager protocol",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[99], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m squad_vocab \u001b[38;5;241m=\u001b[39m create_vocab(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msquad_vocab\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./separate_data/separate_data.question\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./separate_data/separate_data.answer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m squad_pairs \u001b[38;5;241m=\u001b[39m make_pairs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./separate_data/separate_data.question\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./separate_data/separate_data.answer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[1;32mIn[98], line 13\u001b[0m, in \u001b[0;36mmake_pairs\u001b[1;34m(questions_path, answers_path)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_pairs\u001b[39m(questions_path, answers_path):\n\u001b[0;32m     12\u001b[0m     pairs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(questions_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m questions, \\\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28mopen\u001b[39m(answers_path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m answers:\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(questions)):\n\u001b[0;32m     16\u001b[0m             pairs\u001b[38;5;241m.\u001b[39mappend(questions[i], answers[i])\n",
            "\u001b[1;31mTypeError\u001b[0m: 'list' object does not support the context manager protocol"
          ]
        }
      ],
      "source": [
        "squad_vocab = create_vocab('squad_vocab', ['./separate_data/separate_data.question', './separate_data/separate_data.answer'])\n",
        "squad_pairs = make_pairs('./separate_data/separate_data.question', './separate_data/separate_data.answer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "def index_from_sentence(voc:Vocabulary, sentence:str):\n",
        "    return [voc.word2idx[word] for word in sentence.split(' ')]\n",
        "\n",
        "def pad_sentence(sentence:str, max_length = 20):\n",
        "    words = sentence.split(' ')\n",
        "    num_padding = max_length - len(words)\n",
        "    if num_padding > 0:\n",
        "        words += [PAD_tag]*num_padding\n",
        "    return ' '.join([BOS_tag] + words + [EOS_tag])\n",
        "\n",
        "def pad_sentence_batch(sentences, pad_token = PAD_token):\n",
        "    return list(itertools.zip_longest(*sentences, fill_value = pad_token))\n",
        "\n",
        "def pad_mask(sentences, mask_token = PAD_token):\n",
        "    sentences_mask = []\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        sentence_mask = []\n",
        "        for token in sentence:\n",
        "            if token == mask_token:\n",
        "                sentence_mask.append(1)\n",
        "            else:\n",
        "                sentence_mask.append(0)\n",
        "        sentences_mask.append(sentence_mask)\n",
        "    return sentences_mask\n",
        "\n",
        "\n",
        "def batch_input_sentences(sentences, voc: Vocabulary):\n",
        "    '''\n",
        "        1. convert the string sentence into a list of numbers - each number is the index of a word \n",
        "           according to the vocabulary voc.\n",
        "        2. pad these sentences - more description in pad_sentence_batch function\n",
        "        3. return the padded - indexed - batched sentence and the length of individual sentences\n",
        "    '''\n",
        "    indexed_sentences = [index_from_sentence(voc, sentence) for sentence in sentences]\n",
        "    sentence_lengths = [ len(sentence) for sentence in indexed_sentences]\n",
        "    padded_batch = pad_sentence_batch(indexed_sentences)   \n",
        "    return torch.LongTensor(padded_batch), sentence_lengths\n",
        "\n",
        "def batch_output_sentences(sentences, voc: Vocabulary):\n",
        "    indexed_sentences = [index_from_sentence(voc, sentence) for sentence in sentences]\n",
        "    max_sentence_length = max([len(sentence) for sentence in indexed_sentences])\n",
        "    padded_batch = pad_sentence_batch(indexed_sentences)\n",
        "    batch_mask = pad_mask(padded_batch)\n",
        "    batch_mask = torch.BoolTensor(batch_mask)\n",
        "    return torch.LongTensor(padded_batch), max_sentence_length\n",
        "\n",
        "def batch_2_train_data(voc: Vocabulary, pairs):\n",
        "    pairs.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pairs:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    return batch_input_sentences(input_batch, voc), batch_output_sentences(output_batch, voc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[62, 448, 5, 40, 71, 154, 414, 450, 14, 453, 10]"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index_from_sentence(squad_vocab, 'How much did the second world tour make in dollars ?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<BOS> How much did the second world tour make in dollars ? <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <EOS>'"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pad_sentence('How much did the second world tour make in dollars ?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "61071"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "squad_vocab.total_word_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19048"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "squad_vocab.count_high_frequency_words(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "# since seq-seq model is encoder decoder type \n",
        "# we'll be having two model classes - one for encoder and other for decoder\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, num_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers,\n",
        "                          dropout=(0 if num_layers == 1 else dropout), bidirectional=True)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for name, param in self.gru.named_parameters():\n",
        "            if 'weight_ih' in name:\n",
        "                torch.nn.init.xavier_uniform_(param.data)\n",
        "            elif 'weight_hh' in name:\n",
        "                torch.nn.init.orthogonal_(param.data)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        embedded = self.embedding(input_seq)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
        "        return outputs, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.zeros(hidden_size))\n",
        "            nn.init.xavier_uniform_(self.v.data)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        if self.method in ['general', 'concat']:\n",
        "            nn.init.xavier_uniform_(self.attn.weight)\n",
        "            nn.init.constant_(self.attn.bias, 0)\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        attn_energies = attn_energies.t()\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, num_layers=1, dropout=0.1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers, dropout=(0 if num_layers == 1 else dropout))\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.concat.weight)\n",
        "        nn.init.constant_(self.concat.bias, 0)\n",
        "        nn.init.xavier_uniform_(self.out.weight)\n",
        "        nn.init.constant_(self.out.bias, 0)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        output = self.out(concat_output)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "def maskNLLLoss(inp, target, mask, device='cpu'):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)  # Ensure the loss tensor is on the correct device\n",
        "    return loss, nTotal.item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
        "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=20):\n",
        "\n",
        "    # Zero gradients\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # Set device options\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # Lengths for RNN packing should always be on the CPU\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    # Initialize variables\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    # Forward pass through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
        "    decoder_input = torch.LongTensor([[BOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    # Determine if we are using teacher forcing this iteration\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # Forward batch of sequences through decoder one time step at a time\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # Teacher forcing: next input is current target\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # No teacher forcing: next input is decoder's own current output\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    # Perform backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients: gradients are modified in place\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    # Adjust model weights\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
        "\n",
        "    # Load batches for each iteration\n",
        "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "                      for _ in range(n_iteration)]\n",
        "\n",
        "    # Initializations\n",
        "    print('Initializing ...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "    if loadFilename:\n",
        "        start_iteration = checkpoint['iteration'] + 1\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        # Extract fields from batch\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "        # Run a training iteration with batch\n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
        "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "        print_loss += loss\n",
        "\n",
        "        # Print progress\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "            print_loss = 0\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (iteration % save_every == 0):\n",
        "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': voc.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'encoder' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[89], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m save_every \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Ensure dropout layers are in train mode\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m encoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     12\u001b[0m decoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Initialize optimizers\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'encoder' is not defined"
          ]
        }
      ],
      "source": [
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 4000\n",
        "print_every = 1\n",
        "save_every = 500\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "if loadFilename:\n",
        "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "# If you have CUDA, configure CUDA to call\n",
        "for state in encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "for state in decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "# Run training iterations\n",
        "print(\"Starting Training!\")\n",
        "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
        "           print_every, save_every, clip, corpus_name, loadFilename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'encoder' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[91], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Set dropout layers to ``eval`` mode\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m encoder\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      3\u001b[0m decoder\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize search module\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'encoder' is not defined"
          ]
        }
      ],
      "source": [
        "# Set dropout layers to ``eval`` mode\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# Initialize search module\n",
        "searcher = GreedySearchDecoder(encoder, decoder)\n",
        "\n",
        "# Begin chatting (uncomment and run the following line to begin)\n",
        "# evaluateInput(encoder, decoder, searcher, voc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
